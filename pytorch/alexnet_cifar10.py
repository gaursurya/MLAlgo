# -*- coding: utf-8 -*-
"""Alexnet CIFAR10

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SU0YYGnmcRvPtvdL6Mv0X2p-s5jbRfH6
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

batch_size = 256
T=transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])
## Downloading Train dataset
train = torchvision.datasets.CIFAR10('/file/',train=True,download =True,transform = T)
train = torch.utils.data.DataLoader(train,batch_size = batch_size,shuffle=True)


## Downloading Test dataset
test = torchvision.datasets.CIFAR10('/file/',train=False,download =True,transform = T)
test = torch.utils.data.DataLoader(test,batch_size = batch_size ,shuffle=True)

classes = ('plane', 'car', 'bird', 'cat', 'deer', 
           'dog', 'frog', 'horse', 'ship', 'truck')

for index,(images,labels) in enumerate(train):
  print(index,images.size(),len(labels))
  break

class Alexnet(nn.Module):
  def __init__(self):
    super(Alexnet,self).__init__()
    ## Now 2 Conv2d with maxpooling
    self.layer1 = nn.Sequential(nn.Conv2d(in_channels= 3  # RGB channels 
                                          ,out_channels= 96 # Number of kernels
                                          ,kernel_size= 11  # size of kernel 11x11
                                          ,stride = 1),
                                nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),
                                nn.ReLU(),
                                nn.MaxPool2d(stride=2, kernel_size =3))
    
    self.layer2 = nn.Sequential(nn.Conv2d(in_channels = 96 # Always Equals to previous conv2d out_channel
                                          ,out_channels =  256, kernel_size = 5, stride = 2),
                                nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),
                                nn.ReLU(),
                                nn.MaxPool2d(stride=2, kernel_size =3))
    ## Now 3 consecutive Convolutional layer without maxpooling
    self.layer3 = nn.Sequential(nn.Conv2d(in_channels = 256, out_channels = 384, kernel_size = 5, padding = 3),
                                          nn.ReLU())
    
    self.layer4 = nn.Sequential(nn.Conv2d(in_channels = 384, out_channels = 384, kernel_size = 2, padding = 1),
                                nn.ReLU())
    
    self.layer5 = nn.Sequential(nn.Conv2d(in_channels = 384, out_channels = 256, kernel_size = 2, stride=  1),
                                nn.ReLU(),
                                nn.MaxPool2d(stride=1, kernel_size = 2))
    self.layer6 = nn.AdaptiveAvgPool2d((6, 6)) ## Adaptive pooling to match last conv to linear transformation

    self.dropout = nn.Dropout(p=0.5)

    self.fc1 = nn.Linear(256*6*6,4096)
    self.fc2 = nn.Linear(4096,4096)
    self.fc3 = nn.Linear(4096,10) ## 10 is number of class

  def forward(self,x):
    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)
    x = self.layer4(x)
    x = self.layer5(x)
    x = self.layer6(x)
    x = self.dropout(x)
    x = x.view(-1,256*6*6)
    x = self.fc1(x)
    x = self.fc2(x)
    x = self.fc3(x)

    return x

model = Alexnet().to(device)
model

model(images)
#images.size()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from torch.autograd import Variable
# 
# criteria = nn.CrossEntropyLoss()
# optimizer = torch.optim.Adam(params = model.parameters() , lr = .001)
# n_total_steps = len(train)
# num_epochs = 15
# lossess = list()
# for epoch in range(num_epochs):
#   for index, (images, labels) in enumerate(train):
#     images = Variable(images.to(device))
#     labels = Variable(labels.to(device))
#     output = model(images)
# 
#     loss = criteria(output,labels)
#     optimizer.zero_grad()
#     loss.backward()
#     optimizer.step()
# 
#   if epoch % 3 == 0:
#     lossess.append(loss.item())
#     print('Epoch: {} - Loss: {:.6f}'.format(epoch + 1, loss.item()))
# 
# print('Finished Training')

