{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "## Creating Dictionary of Unique Words\n",
    "en_words=set()\n",
    "for line in data.encorder:\n",
    "    for char in line.split():\n",
    "        if char not in en_words:\n",
    "            en_words.add(char)\n",
    "            \n",
    "de_words=set()\n",
    "for line in data.decorder:\n",
    "    for char in line.split():\n",
    "        if char not in de_words:\n",
    "            de_words.add(char)\n",
    "            \n",
    "# Get lists of words :\n",
    "input_words = sorted(list(en_words))\n",
    "target_source_words = sorted(list(de_words))\n",
    "print('Encorder Vocablary size',len(input_words))\n",
    "print('Decorder Vocablary size',len(target_source_words))\n",
    "\n",
    "\n",
    "en_token_to_int_2d = dict()\n",
    "en_int_to_token_2d = dict()\n",
    "\n",
    "de_token_to_int_2d = dict()\n",
    "de_int_to_token_2d= dict()\n",
    "\n",
    "#Tokenizing the words ( Convert them to numbers ) :\n",
    "for i,token in enumerate(input_words):\n",
    "    en_token_to_int_2d[token] = i\n",
    "    en_int_to_token_2d[i]     = token\n",
    "\n",
    "for i,token in enumerate(target_source_words):\n",
    "    de_token_to_int_2d[token] = i\n",
    "    de_int_to_token_2d[i]     = token\n",
    "    \n",
    "\n",
    "    \n",
    "print('length of en_token_to_int_2d',len(en_token_to_int_2d))\n",
    "print('length of de_token_to_int_2d',len(de_token_to_int_2d))\n",
    "\n",
    "de_words_target_3d=set()\n",
    "for line in data.decorder:\n",
    "    for char in line[::]:\n",
    "        if char not in de_words_target_3d:\n",
    "            de_words_target_3d.add(char)\n",
    "            \n",
    "decorder_target_words = sorted(list(de_words_target_3d))\n",
    "\n",
    "\n",
    "de_token_to_int_3d = dict()\n",
    "de_int_to_token_3d = dict()\n",
    "\n",
    "for i,token in enumerate(decorder_target_words):\n",
    "    de_token_to_int_3d[token] = i\n",
    "    de_int_to_token_3d[i]     = token\n",
    "    \n",
    "print('length of de_token_to_int_3d',len(de_token_to_int_3d))\n",
    "\n",
    "\n",
    "## Maximum length data.Encorder/data.Decorder\n",
    "max_len_encorder = list()\n",
    "for char in range(len(data.encorder)):\n",
    "    max_len_encorder.append(len(data.encorder[char]))\n",
    "    \n",
    "max_len_encorder = max(max_len_encorder)\n",
    "\n",
    "max_len_decorder = list()\n",
    "for char in range(len(data.decorder)):\n",
    "    max_len_decorder.append(len(data.decorder[char]))\n",
    "    \n",
    "max_len_decorder = max(max_len_decorder)\n",
    "\n",
    "print('Maximum Length in data.Encorder',max_len_encorder)\n",
    "print('Maximum Length in data.decorder',max_len_decorder)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "def process_comments(list_sentences):\n",
    "    comments = []\n",
    "    for text in (list_sentences):\n",
    "        text = tokenizer.tokenize(text)\n",
    "        comments.append(text)\n",
    "    return comments\n",
    "\n",
    "input_comments = process_comments(list(data[\"encorder\"].values))\n",
    "output_comments = process_comments(list(data[\"decorder\"].values))\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "encoder_input_data = [[en_token_to_int_2d.get(t, 0) for t in comment] \n",
    "             for comment in input_comments[:len(input_comments)]]\n",
    "\n",
    "decoder_input_data = [[de_token_to_int_2d.get(t, 0) for t in comment] \n",
    "             for comment in output_comments[:len(output_comments)]]\n",
    "\n",
    "# padding\n",
    "encoder_input_data = pad_sequences(encoder_input_data, maxlen=max_len_encorder, \n",
    "                     padding=\"post\", truncating=\"post\")\n",
    "\n",
    "decoder_input_data = pad_sequences(decoder_input_data, maxlen=max_len_decorder, \n",
    "                     padding=\"post\", truncating=\"post\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied not inspired from below Source\n"
     ]
    }
   ],
   "source": [
    "print('Copied not inspired from below Source')\n",
    "##https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string,re\n",
    "from string import digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/home/cit/Downloads/fra.txt'\n",
    "data = pd.read_table(file_name, names=['en', 'de'], encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Go.</td>\n",
       "      <td>Va !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Run!</td>\n",
       "      <td>Cours !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Run!</td>\n",
       "      <td>Courez !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Wow!</td>\n",
       "      <td>Ça alors !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Fire!</td>\n",
       "      <td>Au feu !</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      en          de\n",
       "0    Go.        Va !\n",
       "1   Run!     Cours !\n",
       "2   Run!    Courez !\n",
       "3   Wow!  Ça alors !\n",
       "4  Fire!    Au feu !"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.head(10000)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.en=data.en.apply(lambda x: x.lower())\n",
    "data.de=data.de.apply(lambda x: x.lower())\n",
    "\n",
    "# Process commas :\n",
    "data.en=data.en.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "data.de=data.de.apply(lambda x: re.sub(\"'\", '', x)).apply(lambda x: re.sub(\",\", ' COMMA', x))\n",
    "\n",
    "# Getting rid of punctuation\n",
    "exclude = set(string.punctuation)\n",
    "data.en=data.en.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "data.de=data.de.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "# Getting rid of digits\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "data.en=data.en.apply(lambda x: x.translate(remove_digits))\n",
    "data.de=data.de.apply(lambda x: x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending SOS andEOS to target data : \n",
    "data.de = data.de.apply(lambda x : 'SOS_ '+ x + ' _EOS')\n",
    "\n",
    "# Create word dictionaries :\n",
    "en_words=set()\n",
    "for line in data.en:\n",
    "    for char in line.split():\n",
    "        if char not in en_words:\n",
    "            en_words.add(char)\n",
    "    \n",
    "de_words=set()\n",
    "for line in data.de:\n",
    "    for char in line.split():\n",
    "        if char not in de_words:\n",
    "            de_words.add(char)\n",
    "            \n",
    "# get lengths and sizes :\n",
    "num_en_words = len(en_words)\n",
    "num_de_words = len(de_words)\n",
    "\n",
    "max_en_words_per_sample = max([len(sample.split()) for sample in data.en])\n",
    "max_de_words_per_sample = max([len(sample.split()) for sample in data.de])\n",
    "\n",
    "num_en_samples = len(data.en)\n",
    "num_de_samples = len(data.de)\n",
    "\n",
    "# Get lists of words :\n",
    "input_words = sorted(list(en_words))\n",
    "target_words = sorted(list(de_words))\n",
    "\n",
    "en_token_to_int = dict()\n",
    "en_int_to_token = dict()\n",
    "\n",
    "de_token_to_int = dict()\n",
    "de_int_to_token = dict()\n",
    "\n",
    "#Tokenizing the words ( Convert them to numbers ) :\n",
    "for i,token in enumerate(input_words):\n",
    "    en_token_to_int[token] = i\n",
    "    en_int_to_token[i]     = token\n",
    "\n",
    "for i,token in enumerate(target_words):\n",
    "    de_token_to_int[token] = i\n",
    "    de_int_to_token[i]     = token\n",
    "\n",
    "# initiate numpy arrays to hold the data that our seq2seq model will use:\n",
    "encoder_input_data = np.zeros(\n",
    "    (num_en_samples, max_en_words_per_sample),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (num_de_samples, max_de_words_per_sample),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (num_de_samples, max_de_words_per_sample, num_de_words),\n",
    "    dtype='float32')\n",
    "\n",
    "# Process samples, to get input, output, target data:\n",
    "for i, (input_text, target_text) in enumerate(zip(data.en, data.de)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = en_token_to_int[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = de_token_to_int[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, de_token_to_int[word]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 5), (10000, 13), (10000, 13, 4634))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape,decoder_input_data.shape,decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......Encorder......\n"
     ]
    }
   ],
   "source": [
    "print('......Encorder......')\n",
    "hidden_size = 150\n",
    "Laten_dim = 64\n",
    "\n",
    "## Encoprder input and Embedding\n",
    "encorder_input = Input(shape = (None,),name = 'enc_input')\n",
    "encorder_embedding_ix = Embedding(len(en_token_to_int), hidden_size,name = 'enc_emb_ix')  #\n",
    "encorder_embedding =encorder_embedding_ix(encorder_input)\n",
    "\n",
    "## Layer\n",
    "encorder_lstm_ix  = LSTM(Laten_dim,return_state=True ,name = 'enc_lstm_ix')\n",
    "encorder_lstm, state_h, state_c = encorder_lstm_ix(encorder_embedding)\n",
    "\n",
    "## output \n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......Decorder......\n"
     ]
    }
   ],
   "source": [
    "print('......Decorder......')\n",
    "hidden_size = 150\n",
    "Laten_dim = 64\n",
    "\n",
    "## Input and Embedding layers\n",
    "decorder_input = Input(shape = (None,),name = 'dec_input')\n",
    "decorder_embedding_ix= Embedding(len(de_token_to_int) ,hidden_size,name = 'dec_emb_ix') #\n",
    "decorder_embedding = decorder_embedding_ix(decorder_input)\n",
    "\n",
    "## Decorder layers\n",
    "decoder_LSTM_layer_ix = LSTM(Laten_dim ,return_sequences=True , return_state=True,name = 'dec_lstm')\n",
    "decoder_LSTM_layer,_,_ = decoder_LSTM_layer_ix(decorder_embedding , initial_state = encoder_states)\n",
    "\n",
    "## Decorder output\n",
    "decorder_dense = Dense(len(de_token_to_int),activation = 'softmax')\n",
    "decorder_output = decorder_dense(decoder_LSTM_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_input (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_input (InputLayer)          (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_emb_ix (Embedding)          (None, None, 150)    330300      enc_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dec_emb_ix (Embedding)          (None, None, 150)    695100      dec_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_lstm_ix (LSTM)              [(None, 64), (None,  55040       enc_emb_ix[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_lstm (LSTM)                 [(None, None, 64), ( 55040       dec_emb_ix[0][0]                 \n",
      "                                                                 enc_lstm_ix[0][1]                \n",
      "                                                                 enc_lstm_ix[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 4634)   301210      dec_lstm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,436,690\n",
      "Trainable params: 1,436,690\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encorder_input, decorder_input], [decorder_output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cit/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/cit/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 1.8685 - accuracy: 0.0763 - val_loss: 1.9817 - val_accuracy: 0.0769\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 1.5571 - accuracy: 0.0936 - val_loss: 1.8821 - val_accuracy: 0.1000\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 1.4357 - accuracy: 0.1062 - val_loss: 1.8240 - val_accuracy: 0.1072\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 1.3645 - accuracy: 0.1114 - val_loss: 1.7753 - val_accuracy: 0.1118\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 1.3116 - accuracy: 0.1160 - val_loss: 1.7408 - val_accuracy: 0.1178\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 1.2640 - accuracy: 0.1233 - val_loss: 1.7154 - val_accuracy: 0.1257\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 1.2218 - accuracy: 0.1290 - val_loss: 1.6792 - val_accuracy: 0.1283\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 1.1834 - accuracy: 0.1346 - val_loss: 1.6592 - val_accuracy: 0.1358\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 1.1470 - accuracy: 0.1413 - val_loss: 1.6295 - val_accuracy: 0.1416\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 13s 2ms/step - loss: 1.1138 - accuracy: 0.1462 - val_loss: 1.6173 - val_accuracy: 0.1453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc62316ce50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building of Inference Model\n",
    "encorder_model = Model(encorder_input , encoder_states)\n",
    "\n",
    "## dECORDER SETUP\n",
    "decoder_state_input_h = Input(shape=(None,))\n",
    "decoder_state_input_c = Input(shape=(None,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "## decorder Embedding state\n",
    "decorder_embedding2 = decorder_embedding_ix(decorder_input)\n",
    "\n",
    "## To predict the next word in the sequence, set the initial states to the states from the previous time step.\n",
    "decoder_LSTM_layer2,h2,c2 =  decoder_LSTM_layer_ix(decorder_embedding2 ,initial_state = decoder_states_inputs)\n",
    "decoder_state2 = [h2,c2]\n",
    "decorder_output2 = decorder_dense(decoder_LSTM_layer2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decorder_input] + decoder_states_inputs,\n",
    "    [decorder_output2] + decoder_state2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in en_token_to_int.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in de_token_to_int.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encorder_model.predict(input_seq) ## h and c state\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0,0] = de_token_to_int['SOS_'] ## \\t can use <start also>\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :]) ## Not using Teacher Force \n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_EOS' or len(decoded_sentence) > max_de_words_per_sample):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0,0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: go\n",
      "Original State SOS_ va  _EOS\n",
      "Decoded sentence:  prends un maison\n",
      "-\n",
      "Input sentence: run\n",
      "Original State SOS_ cours  _EOS\n",
      "Decoded sentence:  prends _EOS\n",
      "-\n",
      "Input sentence: run\n",
      "Original State SOS_ courez  _EOS\n",
      "Decoded sentence:  prends _EOS\n",
      "-\n",
      "Input sentence: wow\n",
      "Original State SOS_ ça alors  _EOS\n",
      "Decoded sentence:  qui que _EOS\n",
      "-\n",
      "Input sentence: fire\n",
      "Original State SOS_ au feu  _EOS\n",
      "Decoded sentence:  prends un maison\n",
      "-\n",
      "Input sentence: help\n",
      "Original State SOS_ à laide  _EOS\n",
      "Decoded sentence:  prends _EOS\n",
      "-\n",
      "Input sentence: jump\n",
      "Original State SOS_ saute _EOS\n",
      "Decoded sentence:  prends _EOS\n",
      "-\n",
      "Input sentence: stop\n",
      "Original State SOS_ ça suffit  _EOS\n",
      "Decoded sentence:  arrête _EOS\n",
      "-\n",
      "Input sentence: stop\n",
      "Original State SOS_ stop  _EOS\n",
      "Decoded sentence:  arrête _EOS\n",
      "-\n",
      "Input sentence: stop\n",
      "Original State SOS_ arrêtetoi  _EOS\n",
      "Decoded sentence:  arrête _EOS\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', data.en[seq_index])\n",
    "    print('Original State',data.de[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
