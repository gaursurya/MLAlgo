{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pink\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=['I love Banana juice','I love apple juice also','I love Orange juice also','Red is my fav color','I like black',\n",
    "       'I like blue color','I hate fruits specially guava or mango and grapes','Pink is color for girls and black is color for boys',\n",
    "       'green is color for Agriculture','white is color for peace','blue is color for hope']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower(corpus):\n",
    "    corpus=[item.lower() for item in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=lower(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(corpus):\n",
    "    stop_words=['I','i','also','my','is','for','and','or']\n",
    "    results = []\n",
    "    for text in corpus:\n",
    "        tmp = text.split(' ')\n",
    "        for stop_word in stop_words:\n",
    "            if stop_word in tmp:\n",
    "                tmp.remove(stop_word)\n",
    "        results.append(tmp)    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love', 'banana', 'juice'],\n",
       " ['love', 'apple', 'juice'],\n",
       " ['love', 'orange', 'juice'],\n",
       " ['red', 'fav', 'color'],\n",
       " ['like', 'black'],\n",
       " ['like', 'blue', 'color'],\n",
       " ['hate', 'fruits', 'specially', 'guava', 'mango', 'grapes'],\n",
       " ['pink', 'color', 'girls', 'black', 'is', 'color', 'for', 'boys'],\n",
       " ['green', 'color', 'agriculture'],\n",
       " ['white', 'color', 'peace'],\n",
       " ['blue', 'color', 'hope']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus=remove_stop_words(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of Unique words in Corpus\n",
    "words={}\n",
    "for wds in range(len(corpus)):\n",
    "    for w in corpus[wds]:\n",
    "        if w in words:\n",
    "            words[w]=words[w]+1\n",
    "        else:\n",
    "            words[w]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating List Related to the dictonary keys of words\n",
    "merge_list=[]\n",
    "for k,v in words.items():\n",
    "    merge_list.append(k)\n",
    "merge_list=sorted(merge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary with Words2Indexing\n",
    "word2index={merge_list[i]:i for i in range(0,len(merge_list))}\n",
    "##Index2Words\n",
    "Index2Words = dict((v,k) for k,v in word2index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integer Data ['18', '4', '6'] \n",
      "Charater Data ['like', 'blue', 'color']\n"
     ]
    }
   ],
   "source": [
    "### Change the character of strings into Number of index_words\n",
    "text=[]\n",
    "for i in range(len(corpus)):\n",
    "    text.append([])\n",
    "    for xx in corpus[i]:\n",
    "        text[i].append(xx.replace(xx,str(word2index[xx])))\n",
    "        \n",
    "print('Integer Data',text[5],'\\nCharater Data',corpus[5])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Building N-Grams\n",
    "mega_corpus=[]\n",
    "for i in range(len(corpus)):\n",
    "    for xx in corpus[i]:\n",
    "        mega_corpus.append(xx)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['love', 'banana'],\n",
       " ['banana', 'juice'],\n",
       " ['juice', 'love'],\n",
       " ['love', 'apple'],\n",
       " ['apple', 'juice']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Creating Only 2-Grams For Test purposes\n",
    "n=2\n",
    "N_GramModel=[mega_corpus[i:i+n] for i in range(len(mega_corpus)-1)]\n",
    "## Only Considering First 5 Value\n",
    "N_GramModel[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "WINDOW_SIZE = 2\n",
    "for w in corpus:\n",
    "    for word_index, word in enumerate(w):\n",
    "        for nb_word in w[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(w)) + 1] : \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>banana</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love</td>\n",
       "      <td>juice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>banana</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>banana</td>\n",
       "      <td>juice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>juice</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    input   Label\n",
       "0    love  banana\n",
       "1    love   juice\n",
       "2  banana    love\n",
       "3  banana   juice\n",
       "4   juice    love"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(data,columns=['input','Label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating Input data \n",
    "Input=[]\n",
    "for x in df.iloc[:,0]:\n",
    "    Input.append(word2index[x])\n",
    "\n",
    "X_train=[]\n",
    "for k in Input:\n",
    "    temp=np.zeros(len(word2index))\n",
    "    temp[k]=1    \n",
    "    X_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating label data \n",
    "label=[]\n",
    "for x in df.iloc[:,1]:\n",
    "    label.append(word2index[x])\n",
    "    \n",
    "Y_train=[]\n",
    "for k in label:\n",
    "    temp=np.zeros(len(word2index))\n",
    "    temp[k]=1    \n",
    "    Y_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert them to numpy arrays\n",
    "X_train = np.asarray(X_train)\n",
    "Y_train = np.asarray(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1)Both the shapes Array shold be Same \n",
      "2)Column is the unique value present in the corpus \n",
      "3)Rows are the length of dataframe after doing N_Gram\n",
      "Shape of Y_train is (94, 27) \n",
      "Shape of X_Train is (94, 27)\n"
     ]
    }
   ],
   "source": [
    "print('1)Both the shapes Array shold be Same','\\n2)Column is the unique value present in the corpus','\\n3)Rows are the length of dataframe after doing N_Gram')\n",
    "\n",
    "print('Shape of Y_train is',Y_train.shape,'\\nShape of X_Train is',X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## None is their because we don't know the shape of X_train sometimes it us 1*vocab_size  or sometimes it is vocab_size*1\n",
    "vocab_size=len(word2index) ## Always length of a Unique Dictionary\n",
    "x=tf.placeholder(tf.float32, shape=(None,vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## word-Embedding\n",
    "Embed_dim=5\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, Embed_dim],-1,1))\n",
    "b1=tf.Variable(tf.random_normal([Embed_dim],-1,1))\n",
    "hidden_matrix=tf.add((tf.matmul(x,W1)),b1)\n",
    "\n",
    "## out-put layer after softmax\n",
    "W2 = tf.Variable(tf.random_normal([Embed_dim,vocab_size],-1,1))\n",
    "b2=tf.Variable(tf.random_normal([vocab_size],-1,1))\n",
    "\n",
    "### Prediction using Loss Function\n",
    "prediction=tf.nn.softmax((tf.add(tf.matmul(hidden_matrix,W2),b2)))\n",
    "\n",
    "\n",
    "### Creating Cross-Entropy loss Function\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n",
    "\n",
    "## Adam-optimizers\n",
    "train_optimizer = tf.train.AdamOptimizer(0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Data Begins !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 loss is :  8.03913\n",
      "iteration 3000 loss is :  4.530759\n",
      "iteration 6000 loss is :  3.1503572\n",
      "iteration 9000 loss is :  2.472372\n",
      "iteration 12000 loss is :  2.0291445\n",
      "iteration 15000 loss is :  1.7289271\n",
      "iteration 18000 loss is :  1.5495336\n",
      "iteration 21000 loss is :  1.4450225\n",
      "iteration 24000 loss is :  1.3871846\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) \n",
    "\n",
    "iteration = 25000\n",
    "for i in range(iteration):\n",
    "    # input is X_train which is one hot encoded word\n",
    "    # label is Y_train which is one hot encoded neighbor word\n",
    "    sess.run(train_optimizer, feed_dict={x: X_train, y_label: Y_train})\n",
    "    if i % 3000 == 0:\n",
    "        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.3700979  -0.97079873 -2.654842   -1.8231133  -2.0968208 ]\n",
      " [-2.331005    1.4998215   0.74571127 -2.5238714  -1.920799  ]\n",
      " [-2.397241    0.501312    0.05894314 -2.3700252  -1.2363259 ]\n",
      " [ 0.11282279  1.1350769  -1.6619006  -0.859465    0.15081091]\n",
      " [-0.12135387  0.31814733 -2.591023   -3.4166393  -0.6542811 ]\n",
      " [-3.160065   -2.0433376  -3.0914454  -0.53239715  0.50922614]\n",
      " [ 0.34005225 -2.0923467  -0.28079972 -0.37317604 -0.5394984 ]\n",
      " [ 0.8324868  -2.4231892  -2.8673067   1.3878247  -2.9991071 ]\n",
      " [ 0.72472584  1.0234027  -1.1969881   0.8705071  -1.3692926 ]\n",
      " [ 1.3087094  -1.2495974   1.0088114  -1.4078223   1.8136746 ]\n",
      " [-1.9975368  -1.5398446  -1.394702   -2.8620372  -2.0040293 ]\n",
      " [-3.3162386   1.2775453  -0.74179065 -1.9206384   1.903526  ]\n",
      " [-1.3848425  -2.4125936  -2.2718928   0.77027994 -1.089366  ]\n",
      " [-3.3436072  -2.5958495   2.001141   -1.7701273   1.360983  ]\n",
      " [-0.46836463 -3.1949122   1.8129276  -0.6182739   1.8762034 ]\n",
      " [-3.1148636  -4.203465   -3.0103023  -1.3113291  -2.5167053 ]\n",
      " [-1.1611769  -2.7971616  -3.2965245  -1.4761481   1.1306986 ]\n",
      " [-0.02773957  0.5588591   1.7380981  -1.9428166  -1.7998356 ]\n",
      " [-2.5739057  -3.6732063  -2.7010775  -2.0810297  -1.5965071 ]\n",
      " [-1.5681697   1.6735398   1.7306565   0.81537664 -1.7965767 ]\n",
      " [-3.0889554  -1.753803    0.41889188 -3.6111424   0.95400876]\n",
      " [-2.3470569   0.16599621  0.5090911  -2.2095964  -1.4568807 ]\n",
      " [-3.155851   -1.5233674  -2.2235332   1.225177   -3.6943953 ]\n",
      " [ 0.01802081 -0.45279098 -2.5656714  -0.501923    0.86048067]\n",
      " [-1.2590033   0.8555189  -2.2998002   1.5308726   0.81637347]\n",
      " [-0.3502416   0.36444804  1.3653816  -0.16231197  2.1731696 ]\n",
      " [ 1.5977782  -2.172174   -2.760439    0.7514715  -0.6668489 ]]\n",
      "----------\n",
      "[ 0.09214894  0.92301446 -0.20155868  0.2210791   0.07863896]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(W1))\n",
    "print('----------')\n",
    "print(sess.run(b1))\n",
    "print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4622468  -0.04778427 -2.8564005  -1.6020342  -2.0181818 ]\n",
      " [-2.238856    2.422836    0.5441526  -2.3027923  -1.8421601 ]\n",
      " [-2.305092    1.4243264  -0.14261554 -2.148946   -1.157687  ]\n",
      " [ 0.20497173  2.0580914  -1.8634593  -0.6383859   0.22944987]\n",
      " [-0.02920493  1.2411618  -2.7925816  -3.1955602  -0.5756421 ]\n",
      " [-3.067916   -1.1203232  -3.293004   -0.31131804  0.5878651 ]\n",
      " [ 0.43220118 -1.1693323  -0.4823584  -0.15209694 -0.46085942]\n",
      " [ 0.92463577 -1.5001748  -3.0688653   1.6089038  -2.920468  ]\n",
      " [ 0.8168748   1.9464171  -1.3985468   1.0915862  -1.2906537 ]\n",
      " [ 1.4008583  -0.32658297  0.80725265 -1.1867431   1.8923135 ]\n",
      " [-1.9053879  -0.61683017 -1.5962607  -2.640958   -1.9253904 ]\n",
      " [-3.2240896   2.2005599  -0.94334936 -1.6995593   1.9821649 ]\n",
      " [-1.2926936  -1.4895792  -2.4734514   0.99135906 -1.010727  ]\n",
      " [-3.2514582  -1.6728351   1.7995824  -1.5490482   1.4396219 ]\n",
      " [-0.3762157  -2.2718978   1.6113689  -0.3971948   1.9548423 ]\n",
      " [-3.0227146  -3.2804506  -3.211861   -1.09025    -2.4380662 ]\n",
      " [-1.069028   -1.8741472  -3.498083   -1.255069    1.2093375 ]\n",
      " [ 0.06440937  1.4818735   1.5365394  -1.7217375  -1.7211967 ]\n",
      " [-2.4817567  -2.750192   -2.902636   -1.8599505  -1.5178682 ]\n",
      " [-1.4760208   2.5965543   1.5290978   1.0364558  -1.7179378 ]\n",
      " [-2.9968064  -0.83078855  0.2173332  -3.3900633   1.0326477 ]\n",
      " [-2.2549078   1.0890107   0.3075324  -1.9885173  -1.3782418 ]\n",
      " [-3.0637019  -0.60035294 -2.4250917   1.4462562  -3.6157563 ]\n",
      " [ 0.11016975  0.4702235  -2.76723    -0.2808439   0.93911964]\n",
      " [-1.1668544   1.7785333  -2.5013587   1.7519517   0.89501244]\n",
      " [-0.25809267  1.2874625   1.1638229   0.05876713  2.2518086 ]\n",
      " [ 1.6899271  -1.2491596  -2.9619975   0.97255063 -0.5882099 ]]\n"
     ]
    }
   ],
   "source": [
    "vectors = sess.run(W1 + b1)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.305092    1.4243264  -0.14261554 -2.148946   -1.157687  ]\n"
     ]
    }
   ],
   "source": [
    "print(vectors[ word2index['banana'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
