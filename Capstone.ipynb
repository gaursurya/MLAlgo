{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pink\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import tensorflow as tf\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Amazon Reviews.csv')\n",
    "df=df.iloc[0:1000,:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 1000\n"
     ]
    }
   ],
   "source": [
    "## print the Number of Reviews\n",
    "print(\"Number of reviews:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Rating Distribution\n",
    "ax = plt.axes()\n",
    "sns.countplot(df.Score,ax=ax)\n",
    "ax.set_title('Rating Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews--> 780 \n",
      "Number of negeative reviews--> 220\n"
     ]
    }
   ],
   "source": [
    "## Working on positive and negeative reviews\n",
    "df_positive=df.loc[df.Score>3,'Text']\n",
    "df_negeative=df.loc[df.Score<=3,'Text']\n",
    "print('Number of positive reviews-->',len(df_positive),'\\nNumber of negeative reviews-->',len(df_negeative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of positive reviews--> 0.78 \n",
      "Ratio of negeative reviews--> 0.22\n"
     ]
    }
   ],
   "source": [
    "## Ratio of Reviews \n",
    "print('Ratio of positive reviews-->',len(df_positive)/len(df),'\\nRatio of negeative reviews-->',len(df_negeative)/len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    0\n",
       "4    1\n",
       "Name: Review_label, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Review_label']= np.where(df['Score']>3, '1', '0')\n",
    "df['Review_label'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Words in Positive Reviews--> 302995 \n",
      "Total Number of Words in Positive Reviews--> 93856\n"
     ]
    }
   ],
   "source": [
    "postive_review_words=0\n",
    "negative_review_words=0\n",
    "for i in range(len(df)):\n",
    "    if df['Review_label'][i]=='1':\n",
    "        postive_review_words=postive_review_words+len(df['Text'][i])\n",
    "    else:\n",
    "        negative_review_words=negative_review_words+len(df['Text'][i])\n",
    "print('Total Number of Words in Positive Reviews-->',postive_review_words,'\\nTotal Number of Words in Positive Reviews-->', negative_review_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Length of First Sentence\n",
    "len(df['Text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower Case/Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the Length of first paragraph after removing Punctuation--> 260\n"
     ]
    }
   ],
   "source": [
    "## Data Cleansing\n",
    "## Removing Punctuation and Lower-Case all the strings\n",
    "def remove_punc(text):\n",
    "    text=re.sub(r\"[^a-zA-Z0-9]+\",' ',text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text=re.sub(r'<[^>]+>','',text)\n",
    "    \n",
    "    return text.lower()\n",
    "df['Text'] = [remove_punc(df['Text'][x]) for x in range(len(df))]\n",
    "### Finding the Length of first paragraph after removing Punctuation\n",
    "print(('Finding the Length of first paragraph after removing Punctuation-->'),len(df['Text'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the Length of first paragraph after removing Emoticons--> 260\n"
     ]
    }
   ],
   "source": [
    "### Removing Emoticons\n",
    "def remove_punc(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "df['Text']  = [remove_punc(df['Text'][x]) for x in range(len(df))]\n",
    "\n",
    "print(('Finding the Length of first paragraph after removing Emoticons-->'),len(df['Text'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing of Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting it into lists\n",
    "raw_docs_train = df['Text'] .tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1265.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the Length of first paragraph after removing Stop-Words--> 23\n",
      "Length of dataset 1000\n"
     ]
    }
   ],
   "source": [
    "## Removal of Stop Words\n",
    "processed_docs_train_SW = []\n",
    "for w in tqdm(range(len(raw_docs_train))):\n",
    "    tokenizer=nltk.word_tokenize(raw_docs_train[w])\n",
    "    filtered = [word for word in tokenizer if word not in stop_words]     \n",
    "    processed_docs_train_SW.append(filtered)\n",
    "\n",
    "print(('Finding the Length of first paragraph after removing Stop-Words-->'),(len(processed_docs_train_SW[0])))\n",
    "print('Length of dataset',len(processed_docs_train_SW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  if you are looking for the secret ingredient in robitussin i believe i have found it i got this in addition to the root beer extract i ordered which was good and made some cherry soda the flavor is very medicinal \n",
      "-------------------------------------------------------------------------------------------------------\n",
      "After:  looking secret ingredient robitussin believe found got addition root beer extract ordered good made cherry soda flavor medicinal\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: \",df['Text'][3])\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "print(\"After: \",\" \".join(processed_docs_train_SW[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### False Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 49375.54it/s]\n"
     ]
    }
   ],
   "source": [
    "## Find the Length of every words From the corpus\n",
    "len_word={}\n",
    "for w in tqdm(range(len(processed_docs_train_SW))):\n",
    "    for wds in processed_docs_train_SW[w]:\n",
    "           len_word[wds]=len(wds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of false_word is--> 93\n"
     ]
    }
   ],
   "source": [
    "## I am taking v is greater than 25,because Maximum length of english word is 25\n",
    "## Don't remove the word which consists 'no'\n",
    "false_word=list(((k) for k, v in len_word.items() if (v == 1 or (v == 2 and k!='no')or v >= 25 or k=='lol')))\n",
    "print(\"Length of false_word is-->\",len(false_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 13538.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset after false_word is--> 1000\n"
     ]
    }
   ],
   "source": [
    "## Removing Words from False List\n",
    "processed_docs_train_FW=[]\n",
    "for w in tqdm(range(len(processed_docs_train_SW))):\n",
    "    filtered1 = [word for word in processed_docs_train_SW[w] if word not in false_word] \n",
    "    processed_docs_train_FW.append(filtered1)\n",
    "\n",
    "print(\"Length of Dataset after false_word is-->\",len(processed_docs_train_FW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rare Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 70958.80it/s]\n"
     ]
    }
   ],
   "source": [
    "words={}\n",
    "for wds in tqdm(range(len(processed_docs_train_FW))):\n",
    "    for w in processed_docs_train_FW[wds]:\n",
    "        if w in words:\n",
    "            words[w]=words[w]+1\n",
    "        else:\n",
    "            words[w]=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b = dict( (key, value) for (key, value) in a.items() if key == \"hello\" )\n",
    "sorted_words_dict=dict(sorted(words.items(), key=lambda kv: kv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of rare_words is--> 3536\n"
     ]
    }
   ],
   "source": [
    "## getting a Rare Words Having count 1 and 2.\n",
    "rare_words = list(((k) for k, v in sorted_words_dict.items() if (v == 1 or v == 2)))\n",
    "rare_words=sorted(rare_words)\n",
    "print(\"Length of rare_words is-->\",len(rare_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:04<00:00, 242.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset after Rare Words is--> 1000\n"
     ]
    }
   ],
   "source": [
    "### Removing Words which repeats Many Times\n",
    "processed_docs_train_RW=[]\n",
    "for w in tqdm(range(len(processed_docs_train_FW))):\n",
    "    filtered2 = [word for word in processed_docs_train_FW[w] if word not in rare_words] \n",
    "    processed_docs_train_RW.append(filtered2)\n",
    "\n",
    "print(\"Length of Dataset after Rare Words is-->\",len(processed_docs_train_RW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** NEED To UPDATE FREQUENT WORDS ***************\n"
     ]
    }
   ],
   "source": [
    "print('*************** NEED To UPDATE FREQUENT WORDS ***************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of freq_words is--> 2\n"
     ]
    }
   ],
   "source": [
    "#print(sorted_words_dict)\n",
    "freq_words = list(((k) for k, v in sorted_words_dict.items() if (k == 'please' or k=='would')))\n",
    "print(\"Length of freq_words is-->\",len(freq_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 129770.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset after Frequent-Words is--> 1000\n"
     ]
    }
   ],
   "source": [
    "processed_docs_train_af_FW = []\n",
    "for w in tqdm(range(len(processed_docs_train_RW))):\n",
    "    filtered2 = [word for word in processed_docs_train_RW[w] if word not in freq_words] \n",
    "    processed_docs_train_af_FW.append(filtered2)\n",
    "\n",
    "print(\"Length of Dataset after Frequent-Words is-->\",len(processed_docs_train_af_FW))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pink\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#import the nltk package\n",
    "import nltk\n",
    "#call the nltk downloader\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:03<00:00, 273.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Dataset after lemmatization is--> 1000\n"
     ]
    }
   ],
   "source": [
    "processed_docs_train_Lemz=[]\n",
    "for w in tqdm(range(len(processed_docs_train_af_FW))):\n",
    "    lemmatization=[wordnet_lemmatizer.lemmatize(word) for word in processed_docs_train_af_FW[w] ]\n",
    "    #processed_docs_train_Lemz.append(\" \".join(lemmatization))\n",
    "    processed_docs_train_Lemz.append(lemmatization)\n",
    "\n",
    "print(\"Length of Dataset after lemmatization is-->\",len(processed_docs_train_Lemz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  i got a wild hair for taffy and ordered this five pound bag the taffy was all very enjoyable with many flavors watermelon root beer melon peppermint grape etc my only complaint is there was a bit too much red black licorice flavored pieces just not my particular favorites between me my kids and my husband this lasted only two weeks i would recommend this brand of taffy it was a delightful treat \n",
      "-------------------------------------------------------------------------------------------------------\n",
      "After:  got wild hair taffy ordered five pound bag taffy enjoyable many flavors watermelon root beer melon peppermint grape etc complaint bit much red black licorice flavored pieces particular favorites kids husband lasted two weeks would recommend brand taffy delightful treat\n"
     ]
    }
   ],
   "source": [
    "print(\"Before: \",df['Text'][5])\n",
    "print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "print(\"After: \",\" \".join(processed_docs_train_SW[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding (Count Vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 34316.81it/s]\n"
     ]
    }
   ],
   "source": [
    "## Dictionay of unique Words\n",
    "words={}\n",
    "for wds in tqdm(range(len(processed_docs_train_Lemz))):\n",
    "    for w in processed_docs_train_Lemz[wds]:\n",
    "        if w in words:\n",
    "            words[w]=words[w]+1\n",
    "        else:\n",
    "            words[w]=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 235291.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length oc corpus 1000\n"
     ]
    }
   ],
   "source": [
    "## Converting List into Sentences\n",
    "corpus=[]\n",
    "for i in tqdm(range(len(processed_docs_train_Lemz))):\n",
    "    corpus.append(\" \".join(processed_docs_train_Lemz[i]))\n",
    "print('length oc corpus',len(corpus))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Refine_text = pd.DataFrame(corpus,columns=['Refine_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Review_label</th>\n",
       "      <th>Refine_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>i have bought several of the vitality canned d...</td>\n",
       "      <td>1</td>\n",
       "      <td>bought several canned dog food product found g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>product arrived labeled as jumbo salted peanut...</td>\n",
       "      <td>0</td>\n",
       "      <td>product arrived salted peanut peanut actually ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>this is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "      <td>around light citrus nut case cut tiny square c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>if you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "      <td>looking secret ingredient believe found got ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>great taffy at a great price there was a wide ...</td>\n",
       "      <td>1</td>\n",
       "      <td>great taffy great price yummy taffy delivery q...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  i have bought several of the vitality canned d...   \n",
       "1      Not as Advertised  product arrived labeled as jumbo salted peanut...   \n",
       "2  \"Delight\" says it all  this is a confection that has been around a fe...   \n",
       "3         Cough Medicine  if you are looking for the secret ingredient i...   \n",
       "4            Great taffy  great taffy at a great price there was a wide ...   \n",
       "\n",
       "  Review_label                                        Refine_text  \n",
       "0            1  bought several canned dog food product found g...  \n",
       "1            0  product arrived salted peanut peanut actually ...  \n",
       "2            1  around light citrus nut case cut tiny square c...  \n",
       "3            0  looking secret ingredient believe found got ad...  \n",
       "4            1  great taffy great price yummy taffy delivery q...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Concatenateing the two Data-Frames\n",
    "DfData=pd.concat([df, df_Refine_text], axis=1)\n",
    "DfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Target Data\n",
    "df_y=DfData[\"Review_label\"]\n",
    "## Independent Variable\n",
    "df_x=DfData[\"Refine_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************ Count Vectorizer Starts From Here ************************\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "print('************************ Count Vectorizer Starts From Here ************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram_range=(1,2),max_df=1.0,min_df=1,\n",
    "cv=CountVectorizer(stop_words='english',max_features=1200)\n",
    "df_xcv=cv.fit_transform(df_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Documnet one is look like after selecting max_features\n",
      "---------------------------------------------------------------\n",
      "[array(['vendor', 'sure', 'unsalted', 'sized', 'small', 'actually',\n",
      "       'peanut', 'salted', 'arrived', 'product'], dtype='<U14')]\n"
     ]
    }
   ],
   "source": [
    "print('How Documnet one is look like after selecting max_features')\n",
    "print('---------------------------------------------------------------')\n",
    "print(cv.inverse_transform(df_xcv[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- Model Training Starts From Here -----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding\n",
    "from keras.layers import LSTM\n",
    "print(\"------------------------------- Model Training Starts From Here -----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Data-set 670\n",
      "Length of Test Data-set 330\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_xcv, df_y, test_size=0.33, random_state=123)\n",
    "### Convering sparse-matrix into an Array\n",
    "X_train=X_train.toarray()\n",
    "X_test=X_test.toarray()\n",
    "print('Length of Train Data-set',len(X_train))\n",
    "print('Length of Test Data-set',len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 1200, 1)\n"
     ]
    }
   ],
   "source": [
    "X_traincv = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
    "X_traincv=np.array(X_traincv,dtype=np.float32)\n",
    "print(X_traincv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330, 1200, 1)\n"
     ]
    }
   ],
   "source": [
    "X_testcv = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
    "X_testcv=np.array(X_testcv,dtype=np.float32)\n",
    "print(X_testcv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.array(y_train)\n",
    "#y_traincv=np.reshape(y_train.shape,(y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1)                 12        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 14\n",
      "Trainable params: 14\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#embed_dim=128\n",
    "lstm_out=1\n",
    "batch_size=32\n",
    "\n",
    "model=Sequential()\n",
    "#model.add(Embedding(2500,embed_dim,input_length=X_traincv.shape[1],dropout=.2))\n",
    "model.add(LSTM(1,batch_input_shape=(None,1200,1),return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 670 samples, validate on 330 samples\n",
      "Epoch 1/50\n",
      "670/670 [==============================] - ETA: 39s - loss: 0.6930 - acc: 0.37 - ETA: 25s - loss: 0.6929 - acc: 0.57 - ETA: 20s - loss: 0.6929 - acc: 0.61 - ETA: 17s - loss: 0.6927 - acc: 0.66 - ETA: 15s - loss: 0.6923 - acc: 0.68 - ETA: 13s - loss: 0.6921 - acc: 0.70 - ETA: 12s - loss: 0.6921 - acc: 0.70 - ETA: 11s - loss: 0.6918 - acc: 0.71 - ETA: 10s - loss: 0.6916 - acc: 0.72 - ETA: 9s - loss: 0.6913 - acc: 0.7375 - ETA: 8s - loss: 0.6912 - acc: 0.735 - ETA: 7s - loss: 0.6911 - acc: 0.724 - ETA: 6s - loss: 0.6908 - acc: 0.735 - ETA: 5s - loss: 0.6908 - acc: 0.734 - ETA: 4s - loss: 0.6904 - acc: 0.747 - ETA: 3s - loss: 0.6902 - acc: 0.748 - ETA: 3s - loss: 0.6900 - acc: 0.748 - ETA: 2s - loss: 0.6898 - acc: 0.750 - ETA: 1s - loss: 0.6896 - acc: 0.751 - ETA: 0s - loss: 0.6895 - acc: 0.751 - 17s 26ms/step - loss: 0.6892 - acc: 0.7537 - val_loss: 0.6848 - val_acc: 0.8030\n",
      "Epoch 2/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.6860 - acc: 0.75 - ETA: 13s - loss: 0.6845 - acc: 0.79 - ETA: 12s - loss: 0.6825 - acc: 0.84 - ETA: 11s - loss: 0.6834 - acc: 0.81 - ETA: 11s - loss: 0.6838 - acc: 0.79 - ETA: 10s - loss: 0.6844 - acc: 0.76 - ETA: 9s - loss: 0.6845 - acc: 0.7589 - ETA: 9s - loss: 0.6848 - acc: 0.746 - ETA: 8s - loss: 0.6849 - acc: 0.739 - ETA: 7s - loss: 0.6849 - acc: 0.737 - ETA: 7s - loss: 0.6841 - acc: 0.750 - ETA: 6s - loss: 0.6836 - acc: 0.757 - ETA: 5s - loss: 0.6833 - acc: 0.762 - ETA: 4s - loss: 0.6834 - acc: 0.756 - ETA: 4s - loss: 0.6830 - acc: 0.762 - ETA: 3s - loss: 0.6828 - acc: 0.761 - ETA: 2s - loss: 0.6824 - acc: 0.766 - ETA: 2s - loss: 0.6822 - acc: 0.767 - ETA: 1s - loss: 0.6819 - acc: 0.769 - ETA: 0s - loss: 0.6817 - acc: 0.770 - 16s 24ms/step - loss: 0.6817 - acc: 0.7687 - val_loss: 0.6760 - val_acc: 0.8030\n",
      "Epoch 3/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.6772 - acc: 0.78 - ETA: 13s - loss: 0.6762 - acc: 0.79 - ETA: 13s - loss: 0.6757 - acc: 0.80 - ETA: 12s - loss: 0.6750 - acc: 0.80 - ETA: 11s - loss: 0.6747 - acc: 0.80 - ETA: 10s - loss: 0.6758 - acc: 0.78 - ETA: 10s - loss: 0.6764 - acc: 0.77 - ETA: 9s - loss: 0.6761 - acc: 0.7734 - ETA: 8s - loss: 0.6770 - acc: 0.756 - ETA: 7s - loss: 0.6773 - acc: 0.750 - ETA: 7s - loss: 0.6765 - acc: 0.758 - ETA: 6s - loss: 0.6762 - acc: 0.760 - ETA: 5s - loss: 0.6753 - acc: 0.769 - ETA: 5s - loss: 0.6753 - acc: 0.765 - ETA: 4s - loss: 0.6750 - acc: 0.766 - ETA: 3s - loss: 0.6752 - acc: 0.761 - ETA: 2s - loss: 0.6750 - acc: 0.762 - ETA: 2s - loss: 0.6745 - acc: 0.765 - ETA: 1s - loss: 0.6741 - acc: 0.768 - ETA: 0s - loss: 0.6738 - acc: 0.768 - 16s 24ms/step - loss: 0.6736 - acc: 0.7687 - val_loss: 0.6664 - val_acc: 0.8030\n",
      "Epoch 4/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.6649 - acc: 0.81 - ETA: 13s - loss: 0.6647 - acc: 0.81 - ETA: 12s - loss: 0.6655 - acc: 0.80 - ETA: 11s - loss: 0.6680 - acc: 0.77 - ETA: 11s - loss: 0.6670 - acc: 0.78 - ETA: 10s - loss: 0.6663 - acc: 0.78 - ETA: 9s - loss: 0.6665 - acc: 0.7812 - ETA: 9s - loss: 0.6671 - acc: 0.773 - ETA: 8s - loss: 0.6657 - acc: 0.784 - ETA: 7s - loss: 0.6665 - acc: 0.775 - ETA: 7s - loss: 0.6665 - acc: 0.772 - ETA: 6s - loss: 0.6665 - acc: 0.770 - ETA: 5s - loss: 0.6681 - acc: 0.754 - ETA: 4s - loss: 0.6671 - acc: 0.761 - ETA: 4s - loss: 0.6670 - acc: 0.760 - ETA: 3s - loss: 0.6664 - acc: 0.763 - ETA: 2s - loss: 0.6654 - acc: 0.770 - ETA: 1s - loss: 0.6656 - acc: 0.767 - ETA: 1s - loss: 0.6651 - acc: 0.769 - ETA: 0s - loss: 0.6649 - acc: 0.768 - 15s 23ms/step - loss: 0.6646 - acc: 0.7687 - val_loss: 0.6553 - val_acc: 0.8030\n",
      "Epoch 5/50\n",
      "670/670 [==============================] - ETA: 15s - loss: 0.6752 - acc: 0.65 - ETA: 14s - loss: 0.6702 - acc: 0.68 - ETA: 13s - loss: 0.6710 - acc: 0.67 - ETA: 12s - loss: 0.6685 - acc: 0.69 - ETA: 11s - loss: 0.6669 - acc: 0.70 - ETA: 11s - loss: 0.6636 - acc: 0.72 - ETA: 10s - loss: 0.6643 - acc: 0.72 - ETA: 9s - loss: 0.6630 - acc: 0.7305 - ETA: 8s - loss: 0.6593 - acc: 0.753 - ETA: 8s - loss: 0.6596 - acc: 0.750 - ETA: 7s - loss: 0.6595 - acc: 0.750 - ETA: 6s - loss: 0.6576 - acc: 0.760 - ETA: 5s - loss: 0.6577 - acc: 0.757 - ETA: 5s - loss: 0.6571 - acc: 0.758 - ETA: 4s - loss: 0.6567 - acc: 0.760 - ETA: 3s - loss: 0.6566 - acc: 0.759 - ETA: 2s - loss: 0.6565 - acc: 0.759 - ETA: 2s - loss: 0.6573 - acc: 0.753 - ETA: 1s - loss: 0.6565 - acc: 0.756 - ETA: 0s - loss: 0.6550 - acc: 0.764 - 16s 24ms/step - loss: 0.6539 - acc: 0.7687 - val_loss: 0.6424 - val_acc: 0.8030\n",
      "Epoch 6/50\n",
      "670/670 [==============================] - ETA: 11s - loss: 0.6619 - acc: 0.68 - ETA: 11s - loss: 0.6538 - acc: 0.73 - ETA: 10s - loss: 0.6486 - acc: 0.76 - ETA: 10s - loss: 0.6427 - acc: 0.78 - ETA: 10s - loss: 0.6442 - acc: 0.78 - ETA: 9s - loss: 0.6458 - acc: 0.7708 - ETA: 9s - loss: 0.6450 - acc: 0.772 - ETA: 8s - loss: 0.6444 - acc: 0.773 - ETA: 8s - loss: 0.6446 - acc: 0.770 - ETA: 7s - loss: 0.6455 - acc: 0.765 - ETA: 6s - loss: 0.6474 - acc: 0.755 - ETA: 6s - loss: 0.6450 - acc: 0.765 - ETA: 5s - loss: 0.6465 - acc: 0.757 - ETA: 4s - loss: 0.6473 - acc: 0.752 - ETA: 4s - loss: 0.6456 - acc: 0.758 - ETA: 3s - loss: 0.6445 - acc: 0.761 - ETA: 2s - loss: 0.6426 - acc: 0.768 - ETA: 2s - loss: 0.6430 - acc: 0.765 - ETA: 1s - loss: 0.6429 - acc: 0.764 - ETA: 0s - loss: 0.6412 - acc: 0.770 - 15s 23ms/step - loss: 0.6412 - acc: 0.7687 - val_loss: 0.6254 - val_acc: 0.8030\n",
      "Epoch 7/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.6141 - acc: 0.84 - ETA: 12s - loss: 0.6257 - acc: 0.79 - ETA: 11s - loss: 0.6215 - acc: 0.81 - ETA: 10s - loss: 0.6269 - acc: 0.78 - ETA: 10s - loss: 0.6236 - acc: 0.80 - ETA: 9s - loss: 0.6210 - acc: 0.8073 - ETA: 8s - loss: 0.6251 - acc: 0.790 - ETA: 8s - loss: 0.6260 - acc: 0.785 - ETA: 7s - loss: 0.6266 - acc: 0.781 - ETA: 6s - loss: 0.6298 - acc: 0.768 - ETA: 6s - loss: 0.6298 - acc: 0.767 - ETA: 5s - loss: 0.6275 - acc: 0.773 - ETA: 5s - loss: 0.6269 - acc: 0.774 - ETA: 4s - loss: 0.6291 - acc: 0.765 - ETA: 3s - loss: 0.6283 - acc: 0.766 - ETA: 3s - loss: 0.6276 - acc: 0.767 - ETA: 2s - loss: 0.6293 - acc: 0.761 - ETA: 1s - loss: 0.6274 - acc: 0.765 - ETA: 1s - loss: 0.6256 - acc: 0.769 - ETA: 0s - loss: 0.6260 - acc: 0.767 - 16s 23ms/step - loss: 0.6251 - acc: 0.7687 - val_loss: 0.6039 - val_acc: 0.8030\n",
      "Epoch 8/50\n",
      "670/670 [==============================] - ETA: 15s - loss: 0.6002 - acc: 0.81 - ETA: 14s - loss: 0.5994 - acc: 0.81 - ETA: 13s - loss: 0.6135 - acc: 0.77 - ETA: 12s - loss: 0.6122 - acc: 0.77 - ETA: 11s - loss: 0.6111 - acc: 0.77 - ETA: 10s - loss: 0.6082 - acc: 0.78 - ETA: 9s - loss: 0.6162 - acc: 0.7589 - ETA: 9s - loss: 0.6118 - acc: 0.769 - ETA: 8s - loss: 0.6122 - acc: 0.767 - ETA: 7s - loss: 0.6099 - acc: 0.771 - ETA: 7s - loss: 0.6089 - acc: 0.772 - ETA: 6s - loss: 0.6082 - acc: 0.773 - ETA: 5s - loss: 0.6085 - acc: 0.771 - ETA: 4s - loss: 0.6068 - acc: 0.774 - ETA: 4s - loss: 0.6054 - acc: 0.777 - ETA: 3s - loss: 0.6089 - acc: 0.767 - ETA: 2s - loss: 0.6105 - acc: 0.762 - ETA: 2s - loss: 0.6081 - acc: 0.767 - ETA: 1s - loss: 0.6060 - acc: 0.771 - ETA: 0s - loss: 0.6067 - acc: 0.768 - 16s 24ms/step - loss: 0.6064 - acc: 0.7687 - val_loss: 0.5819 - val_acc: 0.8030\n",
      "Epoch 9/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.6200 - acc: 0.71 - ETA: 12s - loss: 0.5910 - acc: 0.78 - ETA: 11s - loss: 0.6098 - acc: 0.73 - ETA: 11s - loss: 0.6228 - acc: 0.71 - ETA: 10s - loss: 0.6189 - acc: 0.71 - ETA: 10s - loss: 0.6212 - acc: 0.71 - ETA: 9s - loss: 0.6142 - acc: 0.7277 - ETA: 8s - loss: 0.6071 - acc: 0.742 - ETA: 8s - loss: 0.6098 - acc: 0.736 - ETA: 7s - loss: 0.6073 - acc: 0.740 - ETA: 6s - loss: 0.6011 - acc: 0.752 - ETA: 6s - loss: 0.5984 - acc: 0.757 - ETA: 5s - loss: 0.5925 - acc: 0.769 - ETA: 4s - loss: 0.5918 - acc: 0.770 - ETA: 4s - loss: 0.5922 - acc: 0.768 - ETA: 3s - loss: 0.5906 - acc: 0.771 - ETA: 2s - loss: 0.5891 - acc: 0.773 - ETA: 2s - loss: 0.5886 - acc: 0.774 - ETA: 1s - loss: 0.5890 - acc: 0.773 - ETA: 0s - loss: 0.5901 - acc: 0.770 - 15s 23ms/step - loss: 0.5908 - acc: 0.7687 - val_loss: 0.5666 - val_acc: 0.8030\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/670 [==============================] - ETA: 14s - loss: 0.5447 - acc: 0.84 - ETA: 13s - loss: 0.5696 - acc: 0.79 - ETA: 13s - loss: 0.5608 - acc: 0.81 - ETA: 11s - loss: 0.5648 - acc: 0.80 - ETA: 11s - loss: 0.5739 - acc: 0.78 - ETA: 10s - loss: 0.5685 - acc: 0.79 - ETA: 9s - loss: 0.5744 - acc: 0.7857 - ETA: 9s - loss: 0.5744 - acc: 0.785 - ETA: 8s - loss: 0.5841 - acc: 0.767 - ETA: 7s - loss: 0.5761 - acc: 0.781 - ETA: 7s - loss: 0.5742 - acc: 0.784 - ETA: 6s - loss: 0.5786 - acc: 0.776 - ETA: 5s - loss: 0.5836 - acc: 0.766 - ETA: 4s - loss: 0.5802 - acc: 0.772 - ETA: 4s - loss: 0.5797 - acc: 0.772 - ETA: 3s - loss: 0.5826 - acc: 0.767 - ETA: 2s - loss: 0.5819 - acc: 0.768 - ETA: 2s - loss: 0.5813 - acc: 0.769 - ETA: 1s - loss: 0.5788 - acc: 0.773 - ETA: 0s - loss: 0.5793 - acc: 0.771 - 16s 24ms/step - loss: 0.5810 - acc: 0.7687 - val_loss: 0.5564 - val_acc: 0.8030\n",
      "Epoch 11/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.5508 - acc: 0.81 - ETA: 13s - loss: 0.5599 - acc: 0.79 - ETA: 12s - loss: 0.5566 - acc: 0.80 - ETA: 11s - loss: 0.5737 - acc: 0.77 - ETA: 10s - loss: 0.5650 - acc: 0.78 - ETA: 9s - loss: 0.5655 - acc: 0.7865 - ETA: 9s - loss: 0.5658 - acc: 0.785 - ETA: 8s - loss: 0.5588 - acc: 0.796 - ETA: 7s - loss: 0.5596 - acc: 0.795 - ETA: 6s - loss: 0.5603 - acc: 0.793 - ETA: 6s - loss: 0.5714 - acc: 0.775 - ETA: 5s - loss: 0.5726 - acc: 0.773 - ETA: 5s - loss: 0.5675 - acc: 0.781 - ETA: 4s - loss: 0.5758 - acc: 0.767 - ETA: 3s - loss: 0.5803 - acc: 0.760 - ETA: 3s - loss: 0.5830 - acc: 0.755 - ETA: 2s - loss: 0.5807 - acc: 0.759 - ETA: 1s - loss: 0.5754 - acc: 0.767 - ETA: 1s - loss: 0.5748 - acc: 0.768 - ETA: 0s - loss: 0.5733 - acc: 0.770 - 15s 22ms/step - loss: 0.5742 - acc: 0.7687 - val_loss: 0.5492 - val_acc: 0.8030\n",
      "Epoch 12/50\n",
      "670/670 [==============================] - ETA: 16s - loss: 0.6235 - acc: 0.68 - ETA: 15s - loss: 0.5932 - acc: 0.73 - ETA: 14s - loss: 0.5965 - acc: 0.72 - ETA: 13s - loss: 0.5830 - acc: 0.75 - ETA: 12s - loss: 0.5788 - acc: 0.75 - ETA: 11s - loss: 0.5862 - acc: 0.74 - ETA: 10s - loss: 0.5886 - acc: 0.74 - ETA: 9s - loss: 0.5826 - acc: 0.7500 - ETA: 8s - loss: 0.5803 - acc: 0.753 - ETA: 8s - loss: 0.5866 - acc: 0.743 - ETA: 7s - loss: 0.5824 - acc: 0.750 - ETA: 6s - loss: 0.5771 - acc: 0.757 - ETA: 5s - loss: 0.5790 - acc: 0.754 - ETA: 5s - loss: 0.5792 - acc: 0.754 - ETA: 4s - loss: 0.5737 - acc: 0.762 - ETA: 3s - loss: 0.5703 - acc: 0.767 - ETA: 2s - loss: 0.5733 - acc: 0.762 - ETA: 2s - loss: 0.5679 - acc: 0.770 - ETA: 1s - loss: 0.5641 - acc: 0.776 - ETA: 0s - loss: 0.5670 - acc: 0.771 - 16s 24ms/step - loss: 0.5691 - acc: 0.7687 - val_loss: 0.5434 - val_acc: 0.8030\n",
      "Epoch 13/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.5583 - acc: 0.78 - ETA: 12s - loss: 0.5796 - acc: 0.75 - ETA: 12s - loss: 0.5724 - acc: 0.76 - ETA: 11s - loss: 0.5741 - acc: 0.75 - ETA: 11s - loss: 0.5837 - acc: 0.74 - ETA: 10s - loss: 0.5829 - acc: 0.74 - ETA: 9s - loss: 0.5699 - acc: 0.7634 - ETA: 8s - loss: 0.5683 - acc: 0.765 - ETA: 8s - loss: 0.5743 - acc: 0.756 - ETA: 7s - loss: 0.5703 - acc: 0.762 - ETA: 6s - loss: 0.5690 - acc: 0.764 - ETA: 6s - loss: 0.5661 - acc: 0.768 - ETA: 5s - loss: 0.5654 - acc: 0.769 - ETA: 4s - loss: 0.5615 - acc: 0.774 - ETA: 4s - loss: 0.5596 - acc: 0.777 - ETA: 3s - loss: 0.5608 - acc: 0.775 - ETA: 2s - loss: 0.5630 - acc: 0.772 - ETA: 2s - loss: 0.5675 - acc: 0.765 - ETA: 1s - loss: 0.5657 - acc: 0.768 - ETA: 0s - loss: 0.5651 - acc: 0.768 - 16s 24ms/step - loss: 0.5651 - acc: 0.7687 - val_loss: 0.5383 - val_acc: 0.8030\n",
      "Epoch 14/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.5541 - acc: 0.78 - ETA: 13s - loss: 0.5878 - acc: 0.73 - ETA: 12s - loss: 0.5689 - acc: 0.76 - ETA: 11s - loss: 0.5651 - acc: 0.76 - ETA: 11s - loss: 0.5718 - acc: 0.75 - ETA: 10s - loss: 0.5650 - acc: 0.76 - ETA: 9s - loss: 0.5535 - acc: 0.7812 - ETA: 9s - loss: 0.5448 - acc: 0.793 - ETA: 8s - loss: 0.5559 - acc: 0.777 - ETA: 7s - loss: 0.5486 - acc: 0.787 - ETA: 6s - loss: 0.5490 - acc: 0.786 - ETA: 6s - loss: 0.5550 - acc: 0.778 - ETA: 5s - loss: 0.5494 - acc: 0.786 - ETA: 4s - loss: 0.5496 - acc: 0.785 - ETA: 4s - loss: 0.5575 - acc: 0.775 - ETA: 3s - loss: 0.5542 - acc: 0.779 - ETA: 2s - loss: 0.5540 - acc: 0.779 - ETA: 2s - loss: 0.5590 - acc: 0.772 - ETA: 1s - loss: 0.5549 - acc: 0.778 - ETA: 0s - loss: 0.5594 - acc: 0.771 - 16s 24ms/step - loss: 0.5617 - acc: 0.7687 - val_loss: 0.5338 - val_acc: 0.8030\n",
      "Epoch 15/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.6212 - acc: 0.68 - ETA: 12s - loss: 0.5739 - acc: 0.75 - ETA: 12s - loss: 0.5502 - acc: 0.78 - ETA: 11s - loss: 0.5620 - acc: 0.76 - ETA: 10s - loss: 0.5453 - acc: 0.78 - ETA: 10s - loss: 0.5619 - acc: 0.76 - ETA: 9s - loss: 0.5636 - acc: 0.7634 - ETA: 8s - loss: 0.5588 - acc: 0.769 - ETA: 8s - loss: 0.5657 - acc: 0.760 - ETA: 7s - loss: 0.5713 - acc: 0.753 - ETA: 6s - loss: 0.5649 - acc: 0.761 - ETA: 6s - loss: 0.5635 - acc: 0.763 - ETA: 5s - loss: 0.5624 - acc: 0.764 - ETA: 4s - loss: 0.5614 - acc: 0.765 - ETA: 4s - loss: 0.5589 - acc: 0.768 - ETA: 3s - loss: 0.5583 - acc: 0.769 - ETA: 2s - loss: 0.5577 - acc: 0.770 - ETA: 2s - loss: 0.5558 - acc: 0.772 - ETA: 1s - loss: 0.5554 - acc: 0.773 - ETA: 0s - loss: 0.5575 - acc: 0.770 - 16s 23ms/step - loss: 0.5587 - acc: 0.7687 - val_loss: 0.5303 - val_acc: 0.8030\n",
      "Epoch 16/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.4739 - acc: 0.87 - ETA: 13s - loss: 0.5352 - acc: 0.79 - ETA: 12s - loss: 0.5146 - acc: 0.82 - ETA: 11s - loss: 0.5289 - acc: 0.80 - ETA: 11s - loss: 0.5127 - acc: 0.82 - ETA: 11s - loss: 0.5183 - acc: 0.81 - ETA: 10s - loss: 0.5259 - acc: 0.80 - ETA: 9s - loss: 0.5129 - acc: 0.8242 - ETA: 8s - loss: 0.5222 - acc: 0.812 - ETA: 8s - loss: 0.5245 - acc: 0.809 - ETA: 7s - loss: 0.5265 - acc: 0.806 - ETA: 6s - loss: 0.5260 - acc: 0.807 - ETA: 5s - loss: 0.5333 - acc: 0.798 - ETA: 5s - loss: 0.5360 - acc: 0.794 - ETA: 4s - loss: 0.5349 - acc: 0.795 - ETA: 3s - loss: 0.5371 - acc: 0.793 - ETA: 2s - loss: 0.5346 - acc: 0.796 - ETA: 2s - loss: 0.5366 - acc: 0.793 - ETA: 1s - loss: 0.5410 - acc: 0.787 - ETA: 0s - loss: 0.5476 - acc: 0.779 - 16s 24ms/step - loss: 0.5565 - acc: 0.7687 - val_loss: 0.5265 - val_acc: 0.8030\n",
      "Epoch 17/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.4165 - acc: 0.93 - ETA: 13s - loss: 0.4932 - acc: 0.84 - ETA: 12s - loss: 0.5444 - acc: 0.78 - ETA: 11s - loss: 0.5444 - acc: 0.78 - ETA: 11s - loss: 0.5494 - acc: 0.77 - ETA: 10s - loss: 0.5614 - acc: 0.76 - ETA: 9s - loss: 0.5479 - acc: 0.7768 - ETA: 8s - loss: 0.5667 - acc: 0.753 - ETA: 8s - loss: 0.5699 - acc: 0.750 - ETA: 7s - loss: 0.5595 - acc: 0.762 - ETA: 6s - loss: 0.5463 - acc: 0.778 - ETA: 6s - loss: 0.5461 - acc: 0.778 - ETA: 5s - loss: 0.5439 - acc: 0.781 - ETA: 4s - loss: 0.5513 - acc: 0.772 - ETA: 4s - loss: 0.5525 - acc: 0.770 - ETA: 3s - loss: 0.5486 - acc: 0.775 - ETA: 2s - loss: 0.5529 - acc: 0.770 - ETA: 2s - loss: 0.5465 - acc: 0.777 - ETA: 1s - loss: 0.5505 - acc: 0.773 - ETA: 0s - loss: 0.5540 - acc: 0.768 - 16s 23ms/step - loss: 0.5540 - acc: 0.7687 - val_loss: 0.5240 - val_acc: 0.8030\n",
      "Epoch 18/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.5425 - acc: 0.78 - ETA: 12s - loss: 0.5029 - acc: 0.82 - ETA: 11s - loss: 0.5160 - acc: 0.81 - ETA: 10s - loss: 0.5490 - acc: 0.77 - ETA: 9s - loss: 0.5529 - acc: 0.7688 - ETA: 8s - loss: 0.5643 - acc: 0.755 - ETA: 7s - loss: 0.5649 - acc: 0.754 - ETA: 7s - loss: 0.5753 - acc: 0.742 - ETA: 6s - loss: 0.5834 - acc: 0.732 - ETA: 5s - loss: 0.5686 - acc: 0.750 - ETA: 5s - loss: 0.5685 - acc: 0.750 - ETA: 4s - loss: 0.5641 - acc: 0.755 - ETA: 4s - loss: 0.5603 - acc: 0.759 - ETA: 3s - loss: 0.5570 - acc: 0.763 - ETA: 3s - loss: 0.5613 - acc: 0.758 - ETA: 2s - loss: 0.5567 - acc: 0.763 - ETA: 2s - loss: 0.5542 - acc: 0.766 - ETA: 1s - loss: 0.5520 - acc: 0.769 - ETA: 1s - loss: 0.5514 - acc: 0.769 - ETA: 0s - loss: 0.5536 - acc: 0.767 - 12s 18ms/step - loss: 0.5523 - acc: 0.7687 - val_loss: 0.5217 - val_acc: 0.8030\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/670 [==============================] - ETA: 9s - loss: 0.6487 - acc: 0.656 - ETA: 9s - loss: 0.5540 - acc: 0.765 - ETA: 8s - loss: 0.5585 - acc: 0.760 - ETA: 8s - loss: 0.5540 - acc: 0.765 - ETA: 7s - loss: 0.5676 - acc: 0.750 - ETA: 7s - loss: 0.5675 - acc: 0.750 - ETA: 6s - loss: 0.5636 - acc: 0.754 - ETA: 6s - loss: 0.5607 - acc: 0.757 - ETA: 5s - loss: 0.5523 - acc: 0.767 - ETA: 5s - loss: 0.5428 - acc: 0.778 - ETA: 4s - loss: 0.5450 - acc: 0.775 - ETA: 4s - loss: 0.5423 - acc: 0.778 - ETA: 3s - loss: 0.5463 - acc: 0.774 - ETA: 3s - loss: 0.5517 - acc: 0.767 - ETA: 2s - loss: 0.5472 - acc: 0.772 - ETA: 2s - loss: 0.5433 - acc: 0.777 - ETA: 1s - loss: 0.5446 - acc: 0.775 - ETA: 1s - loss: 0.5474 - acc: 0.772 - ETA: 0s - loss: 0.5470 - acc: 0.773 - ETA: 0s - loss: 0.5479 - acc: 0.771 - 11s 17ms/step - loss: 0.5508 - acc: 0.7687 - val_loss: 0.5192 - val_acc: 0.8030\n",
      "Epoch 20/50\n",
      "670/670 [==============================] - ETA: 10s - loss: 0.6779 - acc: 0.62 - ETA: 9s - loss: 0.5943 - acc: 0.7188 - ETA: 8s - loss: 0.5850 - acc: 0.729 - ETA: 8s - loss: 0.5595 - acc: 0.757 - ETA: 8s - loss: 0.5776 - acc: 0.737 - ETA: 8s - loss: 0.5711 - acc: 0.744 - ETA: 8s - loss: 0.5624 - acc: 0.754 - ETA: 7s - loss: 0.5664 - acc: 0.750 - ETA: 7s - loss: 0.5570 - acc: 0.760 - ETA: 6s - loss: 0.5495 - acc: 0.768 - ETA: 6s - loss: 0.5536 - acc: 0.764 - ETA: 5s - loss: 0.5499 - acc: 0.768 - ETA: 5s - loss: 0.5577 - acc: 0.759 - ETA: 4s - loss: 0.5623 - acc: 0.754 - ETA: 3s - loss: 0.5607 - acc: 0.756 - ETA: 3s - loss: 0.5628 - acc: 0.753 - ETA: 2s - loss: 0.5596 - acc: 0.757 - ETA: 1s - loss: 0.5600 - acc: 0.756 - ETA: 1s - loss: 0.5573 - acc: 0.759 - ETA: 0s - loss: 0.5535 - acc: 0.764 - 16s 23ms/step - loss: 0.5493 - acc: 0.7687 - val_loss: 0.5174 - val_acc: 0.8030\n",
      "Epoch 21/50\n",
      "670/670 [==============================] - ETA: 15s - loss: 0.5657 - acc: 0.75 - ETA: 14s - loss: 0.5515 - acc: 0.76 - ETA: 13s - loss: 0.5562 - acc: 0.76 - ETA: 13s - loss: 0.5443 - acc: 0.77 - ETA: 12s - loss: 0.5314 - acc: 0.78 - ETA: 11s - loss: 0.5323 - acc: 0.78 - ETA: 10s - loss: 0.5452 - acc: 0.77 - ETA: 9s - loss: 0.5405 - acc: 0.7773 - ETA: 8s - loss: 0.5593 - acc: 0.756 - ETA: 7s - loss: 0.5628 - acc: 0.753 - ETA: 7s - loss: 0.5551 - acc: 0.761 - ETA: 6s - loss: 0.5584 - acc: 0.757 - ETA: 5s - loss: 0.5612 - acc: 0.754 - ETA: 4s - loss: 0.5594 - acc: 0.756 - ETA: 4s - loss: 0.5598 - acc: 0.756 - ETA: 3s - loss: 0.5637 - acc: 0.752 - ETA: 2s - loss: 0.5604 - acc: 0.755 - ETA: 2s - loss: 0.5574 - acc: 0.758 - ETA: 1s - loss: 0.5563 - acc: 0.759 - ETA: 0s - loss: 0.5509 - acc: 0.765 - 16s 24ms/step - loss: 0.5481 - acc: 0.7687 - val_loss: 0.5155 - val_acc: 0.8030\n",
      "Epoch 22/50\n",
      "670/670 [==============================] - ETA: 17s - loss: 0.5650 - acc: 0.75 - ETA: 15s - loss: 0.5504 - acc: 0.76 - ETA: 14s - loss: 0.5455 - acc: 0.77 - ETA: 13s - loss: 0.5503 - acc: 0.76 - ETA: 12s - loss: 0.5474 - acc: 0.76 - ETA: 11s - loss: 0.5552 - acc: 0.76 - ETA: 10s - loss: 0.5523 - acc: 0.76 - ETA: 9s - loss: 0.5576 - acc: 0.7578 - ETA: 8s - loss: 0.5518 - acc: 0.763 - ETA: 8s - loss: 0.5413 - acc: 0.775 - ETA: 7s - loss: 0.5381 - acc: 0.778 - ETA: 6s - loss: 0.5305 - acc: 0.786 - ETA: 5s - loss: 0.5354 - acc: 0.781 - ETA: 5s - loss: 0.5375 - acc: 0.779 - ETA: 4s - loss: 0.5393 - acc: 0.777 - ETA: 3s - loss: 0.5427 - acc: 0.773 - ETA: 2s - loss: 0.5457 - acc: 0.770 - ETA: 2s - loss: 0.5451 - acc: 0.770 - ETA: 1s - loss: 0.5477 - acc: 0.768 - ETA: 0s - loss: 0.5455 - acc: 0.770 - 16s 24ms/step - loss: 0.5471 - acc: 0.7687 - val_loss: 0.5136 - val_acc: 0.8030\n",
      "Epoch 23/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.4449 - acc: 0.87 - ETA: 13s - loss: 0.5494 - acc: 0.76 - ETA: 12s - loss: 0.5344 - acc: 0.78 - ETA: 11s - loss: 0.5568 - acc: 0.75 - ETA: 11s - loss: 0.5463 - acc: 0.76 - ETA: 10s - loss: 0.5443 - acc: 0.77 - ETA: 9s - loss: 0.5429 - acc: 0.7723 - ETA: 8s - loss: 0.5305 - acc: 0.785 - ETA: 7s - loss: 0.5509 - acc: 0.763 - ETA: 7s - loss: 0.5523 - acc: 0.762 - ETA: 6s - loss: 0.5561 - acc: 0.758 - ETA: 6s - loss: 0.5542 - acc: 0.760 - ETA: 5s - loss: 0.5457 - acc: 0.769 - ETA: 4s - loss: 0.5513 - acc: 0.763 - ETA: 3s - loss: 0.5542 - acc: 0.760 - ETA: 3s - loss: 0.5510 - acc: 0.763 - ETA: 2s - loss: 0.5500 - acc: 0.764 - ETA: 1s - loss: 0.5491 - acc: 0.765 - ETA: 1s - loss: 0.5451 - acc: 0.769 - ETA: 0s - loss: 0.5415 - acc: 0.773 - 15s 22ms/step - loss: 0.5461 - acc: 0.7687 - val_loss: 0.5121 - val_acc: 0.8030\n",
      "Epoch 24/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.4726 - acc: 0.84 - ETA: 13s - loss: 0.4878 - acc: 0.82 - ETA: 12s - loss: 0.5029 - acc: 0.81 - ETA: 11s - loss: 0.4952 - acc: 0.82 - ETA: 11s - loss: 0.5273 - acc: 0.78 - ETA: 10s - loss: 0.5181 - acc: 0.79 - ETA: 9s - loss: 0.5158 - acc: 0.7991 - ETA: 8s - loss: 0.5295 - acc: 0.785 - ETA: 8s - loss: 0.5299 - acc: 0.784 - ETA: 7s - loss: 0.5271 - acc: 0.787 - ETA: 6s - loss: 0.5304 - acc: 0.784 - ETA: 6s - loss: 0.5332 - acc: 0.781 - ETA: 5s - loss: 0.5403 - acc: 0.774 - ETA: 4s - loss: 0.5463 - acc: 0.767 - ETA: 4s - loss: 0.5351 - acc: 0.779 - ETA: 3s - loss: 0.5311 - acc: 0.783 - ETA: 2s - loss: 0.5348 - acc: 0.779 - ETA: 1s - loss: 0.5313 - acc: 0.783 - ETA: 1s - loss: 0.5460 - acc: 0.768 - ETA: 0s - loss: 0.5438 - acc: 0.770 - 15s 22ms/step - loss: 0.5454 - acc: 0.7687 - val_loss: 0.5107 - val_acc: 0.8030\n",
      "Epoch 25/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.4703 - acc: 0.84 - ETA: 14s - loss: 0.4392 - acc: 0.87 - ETA: 13s - loss: 0.4806 - acc: 0.83 - ETA: 12s - loss: 0.4857 - acc: 0.82 - ETA: 12s - loss: 0.4950 - acc: 0.81 - ETA: 11s - loss: 0.4908 - acc: 0.82 - ETA: 10s - loss: 0.5101 - acc: 0.80 - ETA: 10s - loss: 0.5206 - acc: 0.79 - ETA: 9s - loss: 0.5254 - acc: 0.7882 - ETA: 8s - loss: 0.5166 - acc: 0.796 - ETA: 7s - loss: 0.5180 - acc: 0.795 - ETA: 6s - loss: 0.5348 - acc: 0.778 - ETA: 5s - loss: 0.5370 - acc: 0.776 - ETA: 5s - loss: 0.5456 - acc: 0.767 - ETA: 4s - loss: 0.5426 - acc: 0.770 - ETA: 3s - loss: 0.5419 - acc: 0.771 - ETA: 2s - loss: 0.5487 - acc: 0.764 - ETA: 2s - loss: 0.5460 - acc: 0.767 - ETA: 1s - loss: 0.5420 - acc: 0.771 - ETA: 0s - loss: 0.5462 - acc: 0.767 - 16s 25ms/step - loss: 0.5447 - acc: 0.7687 - val_loss: 0.5095 - val_acc: 0.8030\n",
      "Epoch 26/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.4685 - acc: 0.84 - ETA: 13s - loss: 0.5158 - acc: 0.79 - ETA: 12s - loss: 0.5210 - acc: 0.79 - ETA: 11s - loss: 0.5078 - acc: 0.80 - ETA: 11s - loss: 0.5126 - acc: 0.80 - ETA: 10s - loss: 0.5210 - acc: 0.79 - ETA: 9s - loss: 0.5179 - acc: 0.7946 - ETA: 9s - loss: 0.5196 - acc: 0.793 - ETA: 8s - loss: 0.5244 - acc: 0.788 - ETA: 7s - loss: 0.5283 - acc: 0.784 - ETA: 6s - loss: 0.5285 - acc: 0.784 - ETA: 6s - loss: 0.5341 - acc: 0.778 - ETA: 5s - loss: 0.5216 - acc: 0.790 - ETA: 4s - loss: 0.5268 - acc: 0.785 - ETA: 4s - loss: 0.5271 - acc: 0.785 - ETA: 3s - loss: 0.5273 - acc: 0.785 - ETA: 2s - loss: 0.5313 - acc: 0.781 - ETA: 2s - loss: 0.5331 - acc: 0.779 - ETA: 1s - loss: 0.5329 - acc: 0.779 - ETA: 0s - loss: 0.5328 - acc: 0.779 - 15s 23ms/step - loss: 0.5441 - acc: 0.7687 - val_loss: 0.5084 - val_acc: 0.8030\n",
      "Epoch 27/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.5628 - acc: 0.75 - ETA: 13s - loss: 0.5147 - acc: 0.79 - ETA: 12s - loss: 0.5093 - acc: 0.80 - ETA: 11s - loss: 0.5307 - acc: 0.78 - ETA: 11s - loss: 0.5307 - acc: 0.78 - ETA: 10s - loss: 0.5414 - acc: 0.77 - ETA: 9s - loss: 0.5261 - acc: 0.7857 - ETA: 9s - loss: 0.5347 - acc: 0.777 - ETA: 8s - loss: 0.5343 - acc: 0.777 - ETA: 7s - loss: 0.5597 - acc: 0.753 - ETA: 6s - loss: 0.5541 - acc: 0.758 - ETA: 6s - loss: 0.5548 - acc: 0.757 - ETA: 5s - loss: 0.5505 - acc: 0.762 - ETA: 4s - loss: 0.5560 - acc: 0.756 - ETA: 4s - loss: 0.5564 - acc: 0.756 - ETA: 3s - loss: 0.5629 - acc: 0.750 - ETA: 2s - loss: 0.5609 - acc: 0.751 - ETA: 2s - loss: 0.5593 - acc: 0.753 - ETA: 1s - loss: 0.5509 - acc: 0.761 - ETA: 0s - loss: 0.5483 - acc: 0.764 - 16s 24ms/step - loss: 0.5435 - acc: 0.7687 - val_loss: 0.5078 - val_acc: 0.8030\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/670 [==============================] - ETA: 13s - loss: 0.4657 - acc: 0.84 - ETA: 12s - loss: 0.5304 - acc: 0.78 - ETA: 12s - loss: 0.5411 - acc: 0.77 - ETA: 11s - loss: 0.5384 - acc: 0.77 - ETA: 10s - loss: 0.5498 - acc: 0.76 - ETA: 10s - loss: 0.5573 - acc: 0.75 - ETA: 9s - loss: 0.5442 - acc: 0.7679 - ETA: 8s - loss: 0.5546 - acc: 0.757 - ETA: 8s - loss: 0.5555 - acc: 0.756 - ETA: 7s - loss: 0.5725 - acc: 0.740 - ETA: 6s - loss: 0.5509 - acc: 0.761 - ETA: 6s - loss: 0.5491 - acc: 0.763 - ETA: 5s - loss: 0.5452 - acc: 0.766 - ETA: 4s - loss: 0.5488 - acc: 0.763 - ETA: 4s - loss: 0.5497 - acc: 0.762 - ETA: 3s - loss: 0.5464 - acc: 0.765 - ETA: 2s - loss: 0.5435 - acc: 0.768 - ETA: 2s - loss: 0.5428 - acc: 0.769 - ETA: 1s - loss: 0.5438 - acc: 0.768 - ETA: 0s - loss: 0.5415 - acc: 0.770 - 16s 24ms/step - loss: 0.5432 - acc: 0.7687 - val_loss: 0.5069 - val_acc: 0.8030\n",
      "Epoch 29/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.4970 - acc: 0.81 - ETA: 13s - loss: 0.5134 - acc: 0.79 - ETA: 13s - loss: 0.5188 - acc: 0.79 - ETA: 12s - loss: 0.5462 - acc: 0.76 - ETA: 11s - loss: 0.5758 - acc: 0.73 - ETA: 11s - loss: 0.5681 - acc: 0.74 - ETA: 10s - loss: 0.5579 - acc: 0.75 - ETA: 9s - loss: 0.5667 - acc: 0.7461 - ETA: 9s - loss: 0.5516 - acc: 0.760 - ETA: 8s - loss: 0.5461 - acc: 0.765 - ETA: 7s - loss: 0.5506 - acc: 0.761 - ETA: 6s - loss: 0.5406 - acc: 0.770 - ETA: 6s - loss: 0.5397 - acc: 0.771 - ETA: 5s - loss: 0.5366 - acc: 0.774 - ETA: 4s - loss: 0.5318 - acc: 0.779 - ETA: 3s - loss: 0.5440 - acc: 0.767 - ETA: 2s - loss: 0.5432 - acc: 0.768 - ETA: 2s - loss: 0.5479 - acc: 0.763 - ETA: 1s - loss: 0.5417 - acc: 0.769 - ETA: 0s - loss: 0.5444 - acc: 0.767 - 17s 25ms/step - loss: 0.5428 - acc: 0.7687 - val_loss: 0.5061 - val_acc: 0.8030\n",
      "Epoch 30/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.4629 - acc: 0.84 - ETA: 13s - loss: 0.4961 - acc: 0.81 - ETA: 12s - loss: 0.4849 - acc: 0.82 - ETA: 12s - loss: 0.5043 - acc: 0.80 - ETA: 11s - loss: 0.5293 - acc: 0.78 - ETA: 10s - loss: 0.5348 - acc: 0.77 - ETA: 9s - loss: 0.5292 - acc: 0.7812 - ETA: 9s - loss: 0.5167 - acc: 0.793 - ETA: 8s - loss: 0.5329 - acc: 0.777 - ETA: 7s - loss: 0.5292 - acc: 0.781 - ETA: 7s - loss: 0.5383 - acc: 0.772 - ETA: 6s - loss: 0.5319 - acc: 0.778 - ETA: 5s - loss: 0.5265 - acc: 0.783 - ETA: 4s - loss: 0.5243 - acc: 0.785 - ETA: 4s - loss: 0.5336 - acc: 0.777 - ETA: 3s - loss: 0.5396 - acc: 0.771 - ETA: 2s - loss: 0.5409 - acc: 0.770 - ETA: 2s - loss: 0.5458 - acc: 0.765 - ETA: 1s - loss: 0.5379 - acc: 0.773 - ETA: 0s - loss: 0.5391 - acc: 0.771 - 16s 24ms/step - loss: 0.5425 - acc: 0.7687 - val_loss: 0.5053 - val_acc: 0.8030\n",
      "Epoch 31/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.5960 - acc: 0.71 - ETA: 12s - loss: 0.5456 - acc: 0.76 - ETA: 12s - loss: 0.5512 - acc: 0.76 - ETA: 11s - loss: 0.5119 - acc: 0.79 - ETA: 11s - loss: 0.5355 - acc: 0.77 - ETA: 10s - loss: 0.5680 - acc: 0.74 - ETA: 9s - loss: 0.5480 - acc: 0.7634 - ETA: 9s - loss: 0.5498 - acc: 0.761 - ETA: 8s - loss: 0.5624 - acc: 0.750 - ETA: 7s - loss: 0.5523 - acc: 0.759 - ETA: 6s - loss: 0.5501 - acc: 0.761 - ETA: 6s - loss: 0.5540 - acc: 0.757 - ETA: 5s - loss: 0.5546 - acc: 0.757 - ETA: 4s - loss: 0.5504 - acc: 0.761 - ETA: 4s - loss: 0.5557 - acc: 0.756 - ETA: 3s - loss: 0.5476 - acc: 0.763 - ETA: 2s - loss: 0.5425 - acc: 0.768 - ETA: 2s - loss: 0.5380 - acc: 0.772 - ETA: 1s - loss: 0.5464 - acc: 0.764 - ETA: 0s - loss: 0.5506 - acc: 0.760 - 16s 24ms/step - loss: 0.5422 - acc: 0.7687 - val_loss: 0.5049 - val_acc: 0.8030\n",
      "Epoch 32/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.5962 - acc: 0.71 - ETA: 13s - loss: 0.6301 - acc: 0.68 - ETA: 12s - loss: 0.6188 - acc: 0.69 - ETA: 11s - loss: 0.5877 - acc: 0.72 - ETA: 11s - loss: 0.5691 - acc: 0.74 - ETA: 10s - loss: 0.5567 - acc: 0.75 - ETA: 9s - loss: 0.5672 - acc: 0.7455 - ETA: 9s - loss: 0.5751 - acc: 0.738 - ETA: 8s - loss: 0.5737 - acc: 0.739 - ETA: 7s - loss: 0.5759 - acc: 0.737 - ETA: 7s - loss: 0.5623 - acc: 0.750 - ETA: 6s - loss: 0.5539 - acc: 0.757 - ETA: 5s - loss: 0.5571 - acc: 0.754 - ETA: 4s - loss: 0.5623 - acc: 0.750 - ETA: 4s - loss: 0.5669 - acc: 0.745 - ETA: 3s - loss: 0.5645 - acc: 0.748 - ETA: 2s - loss: 0.5583 - acc: 0.753 - ETA: 2s - loss: 0.5491 - acc: 0.762 - ETA: 1s - loss: 0.5391 - acc: 0.771 - ETA: 0s - loss: 0.5385 - acc: 0.771 - 16s 24ms/step - loss: 0.5420 - acc: 0.7687 - val_loss: 0.5044 - val_acc: 0.8030\n",
      "Epoch 33/50\n",
      "670/670 [==============================] - ETA: 13s - loss: 0.4941 - acc: 0.81 - ETA: 12s - loss: 0.5111 - acc: 0.79 - ETA: 12s - loss: 0.5282 - acc: 0.78 - ETA: 11s - loss: 0.5196 - acc: 0.78 - ETA: 11s - loss: 0.5213 - acc: 0.78 - ETA: 10s - loss: 0.5339 - acc: 0.77 - ETA: 9s - loss: 0.5282 - acc: 0.7812 - ETA: 9s - loss: 0.5410 - acc: 0.769 - ETA: 8s - loss: 0.5396 - acc: 0.770 - ETA: 7s - loss: 0.5384 - acc: 0.771 - ETA: 7s - loss: 0.5406 - acc: 0.769 - ETA: 6s - loss: 0.5453 - acc: 0.765 - ETA: 5s - loss: 0.5439 - acc: 0.766 - ETA: 5s - loss: 0.5551 - acc: 0.756 - ETA: 4s - loss: 0.5510 - acc: 0.760 - ETA: 3s - loss: 0.5474 - acc: 0.763 - ETA: 2s - loss: 0.5483 - acc: 0.762 - ETA: 2s - loss: 0.5395 - acc: 0.770 - ETA: 1s - loss: 0.5425 - acc: 0.768 - ETA: 0s - loss: 0.5383 - acc: 0.771 - 16s 24ms/step - loss: 0.5419 - acc: 0.7687 - val_loss: 0.5038 - val_acc: 0.8030\n",
      "Epoch 34/50\n",
      "670/670 [==============================] - ETA: 11s - loss: 0.4589 - acc: 0.84 - ETA: 10s - loss: 0.6141 - acc: 0.70 - ETA: 10s - loss: 0.5624 - acc: 0.75 - ETA: 10s - loss: 0.5623 - acc: 0.75 - ETA: 9s - loss: 0.5762 - acc: 0.7375 - ETA: 9s - loss: 0.5681 - acc: 0.744 - ETA: 8s - loss: 0.5673 - acc: 0.745 - ETA: 8s - loss: 0.5710 - acc: 0.742 - ETA: 7s - loss: 0.5662 - acc: 0.746 - ETA: 7s - loss: 0.5554 - acc: 0.756 - ETA: 6s - loss: 0.5435 - acc: 0.767 - ETA: 5s - loss: 0.5480 - acc: 0.763 - ETA: 5s - loss: 0.5437 - acc: 0.766 - ETA: 4s - loss: 0.5426 - acc: 0.767 - ETA: 3s - loss: 0.5485 - acc: 0.762 - ETA: 3s - loss: 0.5516 - acc: 0.759 - ETA: 2s - loss: 0.5481 - acc: 0.762 - ETA: 1s - loss: 0.5431 - acc: 0.767 - ETA: 1s - loss: 0.5423 - acc: 0.768 - ETA: 0s - loss: 0.5433 - acc: 0.767 - 15s 23ms/step - loss: 0.5417 - acc: 0.7687 - val_loss: 0.5034 - val_acc: 0.8030\n",
      "Epoch 35/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.5624 - acc: 0.75 - ETA: 13s - loss: 0.4929 - acc: 0.81 - ETA: 12s - loss: 0.4929 - acc: 0.81 - ETA: 12s - loss: 0.5016 - acc: 0.80 - ETA: 11s - loss: 0.5276 - acc: 0.78 - ETA: 10s - loss: 0.5102 - acc: 0.79 - ETA: 10s - loss: 0.4928 - acc: 0.81 - ETA: 9s - loss: 0.4971 - acc: 0.8086 - ETA: 8s - loss: 0.5160 - acc: 0.791 - ETA: 7s - loss: 0.5276 - acc: 0.781 - ETA: 7s - loss: 0.5308 - acc: 0.778 - ETA: 6s - loss: 0.5276 - acc: 0.781 - ETA: 5s - loss: 0.5303 - acc: 0.778 - ETA: 5s - loss: 0.5376 - acc: 0.772 - ETA: 4s - loss: 0.5299 - acc: 0.779 - ETA: 3s - loss: 0.5385 - acc: 0.771 - ETA: 2s - loss: 0.5461 - acc: 0.764 - ETA: 2s - loss: 0.5450 - acc: 0.765 - ETA: 1s - loss: 0.5422 - acc: 0.768 - ETA: 0s - loss: 0.5345 - acc: 0.775 - 16s 24ms/step - loss: 0.5416 - acc: 0.7687 - val_loss: 0.5029 - val_acc: 0.8030\n",
      "Epoch 36/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.4923 - acc: 0.81 - ETA: 13s - loss: 0.4572 - acc: 0.84 - ETA: 12s - loss: 0.5040 - acc: 0.80 - ETA: 12s - loss: 0.5361 - acc: 0.77 - ETA: 11s - loss: 0.5484 - acc: 0.76 - ETA: 10s - loss: 0.5390 - acc: 0.77 - ETA: 10s - loss: 0.5424 - acc: 0.76 - ETA: 9s - loss: 0.5361 - acc: 0.7734 - ETA: 8s - loss: 0.5390 - acc: 0.770 - ETA: 7s - loss: 0.5308 - acc: 0.778 - ETA: 7s - loss: 0.5337 - acc: 0.775 - ETA: 6s - loss: 0.5273 - acc: 0.781 - ETA: 5s - loss: 0.5300 - acc: 0.778 - ETA: 4s - loss: 0.5222 - acc: 0.785 - ETA: 4s - loss: 0.5320 - acc: 0.777 - ETA: 3s - loss: 0.5361 - acc: 0.773 - ETA: 2s - loss: 0.5356 - acc: 0.773 - ETA: 2s - loss: 0.5370 - acc: 0.772 - ETA: 1s - loss: 0.5402 - acc: 0.769 - ETA: 0s - loss: 0.5378 - acc: 0.771 - 16s 24ms/step - loss: 0.5414 - acc: 0.7687 - val_loss: 0.5025 - val_acc: 0.8030\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/670 [==============================] - ETA: 14s - loss: 0.4566 - acc: 0.84 - ETA: 13s - loss: 0.5448 - acc: 0.76 - ETA: 12s - loss: 0.5742 - acc: 0.73 - ETA: 12s - loss: 0.5448 - acc: 0.76 - ETA: 11s - loss: 0.5554 - acc: 0.75 - ETA: 10s - loss: 0.5389 - acc: 0.77 - ETA: 10s - loss: 0.5221 - acc: 0.78 - ETA: 9s - loss: 0.5316 - acc: 0.7773 - ETA: 8s - loss: 0.5389 - acc: 0.770 - ETA: 8s - loss: 0.5165 - acc: 0.790 - ETA: 7s - loss: 0.5143 - acc: 0.792 - ETA: 6s - loss: 0.5124 - acc: 0.794 - ETA: 5s - loss: 0.5108 - acc: 0.795 - ETA: 5s - loss: 0.5145 - acc: 0.792 - ETA: 4s - loss: 0.5177 - acc: 0.789 - ETA: 3s - loss: 0.5271 - acc: 0.781 - ETA: 2s - loss: 0.5313 - acc: 0.777 - ETA: 2s - loss: 0.5330 - acc: 0.776 - ETA: 1s - loss: 0.5364 - acc: 0.773 - ETA: 0s - loss: 0.5431 - acc: 0.767 - 17s 25ms/step - loss: 0.5414 - acc: 0.7687 - val_loss: 0.5022 - val_acc: 0.8030\n",
      "Epoch 38/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.4915 - acc: 0.81 - ETA: 13s - loss: 0.4737 - acc: 0.82 - ETA: 13s - loss: 0.5388 - acc: 0.77 - ETA: 12s - loss: 0.5447 - acc: 0.76 - ETA: 11s - loss: 0.5554 - acc: 0.75 - ETA: 10s - loss: 0.5506 - acc: 0.76 - ETA: 10s - loss: 0.5574 - acc: 0.75 - ETA: 9s - loss: 0.5447 - acc: 0.7656 - ETA: 8s - loss: 0.5467 - acc: 0.763 - ETA: 7s - loss: 0.5518 - acc: 0.759 - ETA: 7s - loss: 0.5366 - acc: 0.772 - ETA: 6s - loss: 0.5418 - acc: 0.768 - ETA: 5s - loss: 0.5433 - acc: 0.766 - ETA: 5s - loss: 0.5320 - acc: 0.776 - ETA: 4s - loss: 0.5269 - acc: 0.781 - ETA: 3s - loss: 0.5314 - acc: 0.777 - ETA: 2s - loss: 0.5332 - acc: 0.775 - ETA: 2s - loss: 0.5408 - acc: 0.769 - ETA: 1s - loss: 0.5344 - acc: 0.774 - ETA: 0s - loss: 0.5323 - acc: 0.776 - 16s 24ms/step - loss: 0.5413 - acc: 0.7687 - val_loss: 0.5020 - val_acc: 0.8030\n",
      "Epoch 39/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.6338 - acc: 0.68 - ETA: 13s - loss: 0.5269 - acc: 0.78 - ETA: 13s - loss: 0.5150 - acc: 0.79 - ETA: 12s - loss: 0.5358 - acc: 0.77 - ETA: 11s - loss: 0.5197 - acc: 0.78 - ETA: 10s - loss: 0.5150 - acc: 0.79 - ETA: 10s - loss: 0.5166 - acc: 0.79 - ETA: 9s - loss: 0.5224 - acc: 0.7852 - ETA: 8s - loss: 0.5149 - acc: 0.791 - ETA: 7s - loss: 0.5125 - acc: 0.793 - ETA: 7s - loss: 0.5171 - acc: 0.789 - ETA: 6s - loss: 0.5268 - acc: 0.781 - ETA: 5s - loss: 0.5241 - acc: 0.783 - ETA: 4s - loss: 0.5243 - acc: 0.783 - ETA: 4s - loss: 0.5364 - acc: 0.772 - ETA: 3s - loss: 0.5291 - acc: 0.779 - ETA: 2s - loss: 0.5352 - acc: 0.773 - ETA: 2s - loss: 0.5348 - acc: 0.774 - ETA: 1s - loss: 0.5325 - acc: 0.776 - ETA: 0s - loss: 0.5393 - acc: 0.770 - 16s 24ms/step - loss: 0.5412 - acc: 0.7687 - val_loss: 0.5017 - val_acc: 0.8030\n",
      "Epoch 40/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.4550 - acc: 0.84 - ETA: 13s - loss: 0.3833 - acc: 0.90 - ETA: 13s - loss: 0.4550 - acc: 0.84 - ETA: 12s - loss: 0.4730 - acc: 0.82 - ETA: 11s - loss: 0.5196 - acc: 0.78 - ETA: 10s - loss: 0.5387 - acc: 0.77 - ETA: 10s - loss: 0.5370 - acc: 0.77 - ETA: 9s - loss: 0.5312 - acc: 0.7773 - ETA: 8s - loss: 0.5387 - acc: 0.770 - ETA: 7s - loss: 0.5339 - acc: 0.775 - ETA: 7s - loss: 0.5463 - acc: 0.764 - ETA: 6s - loss: 0.5536 - acc: 0.757 - ETA: 5s - loss: 0.5460 - acc: 0.764 - ETA: 5s - loss: 0.5472 - acc: 0.763 - ETA: 4s - loss: 0.5411 - acc: 0.768 - ETA: 3s - loss: 0.5491 - acc: 0.761 - ETA: 2s - loss: 0.5436 - acc: 0.766 - ETA: 2s - loss: 0.5407 - acc: 0.769 - ETA: 1s - loss: 0.5418 - acc: 0.768 - ETA: 0s - loss: 0.5429 - acc: 0.767 - 17s 25ms/step - loss: 0.5412 - acc: 0.7687 - val_loss: 0.5016 - val_acc: 0.8030\n",
      "Epoch 41/50\n",
      "670/670 [==============================] - ETA: 15s - loss: 0.5267 - acc: 0.78 - ETA: 14s - loss: 0.4728 - acc: 0.82 - ETA: 13s - loss: 0.4788 - acc: 0.82 - ETA: 12s - loss: 0.5177 - acc: 0.78 - ETA: 12s - loss: 0.5195 - acc: 0.78 - ETA: 11s - loss: 0.5327 - acc: 0.77 - ETA: 10s - loss: 0.5318 - acc: 0.77 - ETA: 9s - loss: 0.5491 - acc: 0.7617 - ETA: 9s - loss: 0.5386 - acc: 0.770 - ETA: 8s - loss: 0.5338 - acc: 0.775 - ETA: 7s - loss: 0.5266 - acc: 0.781 - ETA: 7s - loss: 0.5236 - acc: 0.783 - ETA: 6s - loss: 0.5239 - acc: 0.783 - ETA: 5s - loss: 0.5318 - acc: 0.776 - ETA: 4s - loss: 0.5386 - acc: 0.770 - ETA: 3s - loss: 0.5334 - acc: 0.775 - ETA: 3s - loss: 0.5393 - acc: 0.770 - ETA: 2s - loss: 0.5386 - acc: 0.770 - ETA: 1s - loss: 0.5380 - acc: 0.771 - ETA: 0s - loss: 0.5392 - acc: 0.770 - 18s 26ms/step - loss: 0.5411 - acc: 0.7687 - val_loss: 0.5014 - val_acc: 0.8030\n",
      "Epoch 42/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.5265 - acc: 0.78 - ETA: 14s - loss: 0.4904 - acc: 0.81 - ETA: 14s - loss: 0.5265 - acc: 0.78 - ETA: 13s - loss: 0.5536 - acc: 0.75 - ETA: 13s - loss: 0.5554 - acc: 0.75 - ETA: 12s - loss: 0.5626 - acc: 0.75 - ETA: 11s - loss: 0.5523 - acc: 0.75 - ETA: 10s - loss: 0.5446 - acc: 0.76 - ETA: 9s - loss: 0.5265 - acc: 0.7812 - ETA: 8s - loss: 0.5374 - acc: 0.771 - ETA: 7s - loss: 0.5397 - acc: 0.769 - ETA: 7s - loss: 0.5416 - acc: 0.768 - ETA: 6s - loss: 0.5376 - acc: 0.771 - ETA: 5s - loss: 0.5317 - acc: 0.776 - ETA: 4s - loss: 0.5362 - acc: 0.772 - ETA: 3s - loss: 0.5401 - acc: 0.769 - ETA: 3s - loss: 0.5457 - acc: 0.764 - ETA: 2s - loss: 0.5526 - acc: 0.758 - ETA: 1s - loss: 0.5436 - acc: 0.766 - ETA: 0s - loss: 0.5410 - acc: 0.768 - 17s 26ms/step - loss: 0.5411 - acc: 0.7687 - val_loss: 0.5012 - val_acc: 0.8030\n",
      "Epoch 43/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.5627 - acc: 0.75 - ETA: 14s - loss: 0.5627 - acc: 0.75 - ETA: 13s - loss: 0.5627 - acc: 0.75 - ETA: 12s - loss: 0.5717 - acc: 0.74 - ETA: 11s - loss: 0.5482 - acc: 0.76 - ETA: 11s - loss: 0.5204 - acc: 0.78 - ETA: 10s - loss: 0.5161 - acc: 0.79 - ETA: 9s - loss: 0.5310 - acc: 0.7773 - ETA: 8s - loss: 0.5265 - acc: 0.781 - ETA: 8s - loss: 0.5156 - acc: 0.790 - ETA: 7s - loss: 0.5199 - acc: 0.786 - ETA: 6s - loss: 0.5234 - acc: 0.783 - ETA: 5s - loss: 0.5320 - acc: 0.776 - ETA: 5s - loss: 0.5394 - acc: 0.770 - ETA: 4s - loss: 0.5386 - acc: 0.770 - ETA: 3s - loss: 0.5423 - acc: 0.767 - ETA: 2s - loss: 0.5371 - acc: 0.772 - ETA: 2s - loss: 0.5466 - acc: 0.763 - ETA: 1s - loss: 0.5398 - acc: 0.769 - ETA: 0s - loss: 0.5410 - acc: 0.768 - 16s 25ms/step - loss: 0.5411 - acc: 0.7687 - val_loss: 0.5010 - val_acc: 0.8030\n",
      "Epoch 44/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.5264 - acc: 0.78 - ETA: 13s - loss: 0.4537 - acc: 0.84 - ETA: 13s - loss: 0.5022 - acc: 0.80 - ETA: 12s - loss: 0.5173 - acc: 0.78 - ETA: 11s - loss: 0.5118 - acc: 0.79 - ETA: 10s - loss: 0.5325 - acc: 0.77 - ETA: 10s - loss: 0.5264 - acc: 0.78 - ETA: 9s - loss: 0.5036 - acc: 0.8008 - ETA: 8s - loss: 0.5021 - acc: 0.802 - ETA: 8s - loss: 0.5082 - acc: 0.796 - ETA: 7s - loss: 0.5165 - acc: 0.789 - ETA: 6s - loss: 0.5142 - acc: 0.791 - ETA: 5s - loss: 0.5180 - acc: 0.788 - ETA: 5s - loss: 0.5238 - acc: 0.783 - ETA: 4s - loss: 0.5361 - acc: 0.772 - ETA: 3s - loss: 0.5355 - acc: 0.773 - ETA: 2s - loss: 0.5328 - acc: 0.775 - ETA: 2s - loss: 0.5324 - acc: 0.776 - ETA: 1s - loss: 0.5379 - acc: 0.771 - ETA: 0s - loss: 0.5355 - acc: 0.773 - 16s 24ms/step - loss: 0.5411 - acc: 0.7687 - val_loss: 0.5008 - val_acc: 0.8030\n",
      "Epoch 45/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.4898 - acc: 0.81 - ETA: 14s - loss: 0.5628 - acc: 0.75 - ETA: 13s - loss: 0.5871 - acc: 0.72 - ETA: 12s - loss: 0.5810 - acc: 0.73 - ETA: 11s - loss: 0.5774 - acc: 0.73 - ETA: 11s - loss: 0.5749 - acc: 0.73 - ETA: 10s - loss: 0.5628 - acc: 0.75 - ETA: 9s - loss: 0.5628 - acc: 0.7500 - ETA: 9s - loss: 0.5790 - acc: 0.736 - ETA: 8s - loss: 0.5810 - acc: 0.734 - ETA: 7s - loss: 0.5661 - acc: 0.747 - ETA: 7s - loss: 0.5598 - acc: 0.752 - ETA: 6s - loss: 0.5656 - acc: 0.747 - ETA: 5s - loss: 0.5680 - acc: 0.745 - ETA: 4s - loss: 0.5579 - acc: 0.754 - ETA: 3s - loss: 0.5469 - acc: 0.763 - ETA: 3s - loss: 0.5435 - acc: 0.766 - ETA: 2s - loss: 0.5365 - acc: 0.772 - ETA: 1s - loss: 0.5321 - acc: 0.776 - ETA: 0s - loss: 0.5300 - acc: 0.778 - 17s 26ms/step - loss: 0.5411 - acc: 0.7687 - val_loss: 0.5009 - val_acc: 0.8030\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "670/670 [==============================] - ETA: 15s - loss: 0.5628 - acc: 0.75 - ETA: 14s - loss: 0.5993 - acc: 0.71 - ETA: 13s - loss: 0.5506 - acc: 0.76 - ETA: 12s - loss: 0.5445 - acc: 0.76 - ETA: 12s - loss: 0.5044 - acc: 0.80 - ETA: 11s - loss: 0.5263 - acc: 0.78 - ETA: 10s - loss: 0.5419 - acc: 0.76 - ETA: 9s - loss: 0.5400 - acc: 0.7695 - ETA: 8s - loss: 0.5303 - acc: 0.777 - ETA: 8s - loss: 0.5446 - acc: 0.765 - ETA: 7s - loss: 0.5462 - acc: 0.764 - ETA: 6s - loss: 0.5415 - acc: 0.768 - ETA: 5s - loss: 0.5291 - acc: 0.778 - ETA: 5s - loss: 0.5367 - acc: 0.772 - ETA: 4s - loss: 0.5385 - acc: 0.770 - ETA: 3s - loss: 0.5331 - acc: 0.775 - ETA: 2s - loss: 0.5349 - acc: 0.773 - ETA: 2s - loss: 0.5364 - acc: 0.772 - ETA: 1s - loss: 0.5340 - acc: 0.774 - ETA: 0s - loss: 0.5372 - acc: 0.771 - 17s 25ms/step - loss: 0.5410 - acc: 0.7687 - val_loss: 0.5007 - val_acc: 0.8030\n",
      "Epoch 47/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.4896 - acc: 0.81 - ETA: 13s - loss: 0.5262 - acc: 0.78 - ETA: 12s - loss: 0.5262 - acc: 0.78 - ETA: 11s - loss: 0.5170 - acc: 0.78 - ETA: 11s - loss: 0.5115 - acc: 0.79 - ETA: 10s - loss: 0.5079 - acc: 0.79 - ETA: 9s - loss: 0.5210 - acc: 0.7857 - ETA: 9s - loss: 0.5354 - acc: 0.773 - ETA: 8s - loss: 0.5221 - acc: 0.784 - ETA: 7s - loss: 0.5262 - acc: 0.781 - ETA: 7s - loss: 0.5329 - acc: 0.775 - ETA: 6s - loss: 0.5292 - acc: 0.778 - ETA: 5s - loss: 0.5318 - acc: 0.776 - ETA: 5s - loss: 0.5419 - acc: 0.767 - ETA: 4s - loss: 0.5384 - acc: 0.770 - ETA: 3s - loss: 0.5285 - acc: 0.779 - ETA: 2s - loss: 0.5413 - acc: 0.768 - ETA: 2s - loss: 0.5384 - acc: 0.770 - ETA: 1s - loss: 0.5436 - acc: 0.766 - ETA: 0s - loss: 0.5427 - acc: 0.767 - 16s 25ms/step - loss: 0.5410 - acc: 0.7687 - val_loss: 0.5006 - val_acc: 0.8030\n",
      "Epoch 48/50\n",
      "670/670 [==============================] - ETA: 15s - loss: 0.4894 - acc: 0.81 - ETA: 14s - loss: 0.4711 - acc: 0.82 - ETA: 13s - loss: 0.4894 - acc: 0.81 - ETA: 12s - loss: 0.4894 - acc: 0.81 - ETA: 11s - loss: 0.5041 - acc: 0.80 - ETA: 11s - loss: 0.5262 - acc: 0.78 - ETA: 10s - loss: 0.5209 - acc: 0.78 - ETA: 9s - loss: 0.5032 - acc: 0.8008 - ETA: 8s - loss: 0.4976 - acc: 0.805 - ETA: 8s - loss: 0.4967 - acc: 0.806 - ETA: 7s - loss: 0.4994 - acc: 0.804 - ETA: 6s - loss: 0.5231 - acc: 0.783 - ETA: 5s - loss: 0.5290 - acc: 0.778 - ETA: 4s - loss: 0.5288 - acc: 0.779 - ETA: 4s - loss: 0.5360 - acc: 0.772 - ETA: 3s - loss: 0.5469 - acc: 0.763 - ETA: 2s - loss: 0.5522 - acc: 0.759 - ETA: 2s - loss: 0.5487 - acc: 0.762 - ETA: 1s - loss: 0.5417 - acc: 0.768 - ETA: 0s - loss: 0.5391 - acc: 0.770 - 16s 23ms/step - loss: 0.5410 - acc: 0.7687 - val_loss: 0.5005 - val_acc: 0.8030\n",
      "Epoch 49/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.5261 - acc: 0.78 - ETA: 13s - loss: 0.5813 - acc: 0.73 - ETA: 13s - loss: 0.5507 - acc: 0.76 - ETA: 12s - loss: 0.5997 - acc: 0.71 - ETA: 11s - loss: 0.5850 - acc: 0.73 - ETA: 10s - loss: 0.5874 - acc: 0.72 - ETA: 10s - loss: 0.5839 - acc: 0.73 - ETA: 9s - loss: 0.5721 - acc: 0.7422 - ETA: 9s - loss: 0.5629 - acc: 0.750 - ETA: 8s - loss: 0.5556 - acc: 0.756 - ETA: 7s - loss: 0.5529 - acc: 0.758 - ETA: 6s - loss: 0.5507 - acc: 0.760 - ETA: 6s - loss: 0.5488 - acc: 0.762 - ETA: 5s - loss: 0.5472 - acc: 0.763 - ETA: 4s - loss: 0.5458 - acc: 0.764 - ETA: 3s - loss: 0.5377 - acc: 0.771 - ETA: 2s - loss: 0.5348 - acc: 0.773 - ETA: 2s - loss: 0.5405 - acc: 0.769 - ETA: 1s - loss: 0.5378 - acc: 0.771 - ETA: 0s - loss: 0.5372 - acc: 0.771 - 17s 25ms/step - loss: 0.5410 - acc: 0.7687 - val_loss: 0.5005 - val_acc: 0.8030\n",
      "Epoch 50/50\n",
      "670/670 [==============================] - ETA: 14s - loss: 0.5997 - acc: 0.71 - ETA: 13s - loss: 0.5077 - acc: 0.79 - ETA: 13s - loss: 0.5261 - acc: 0.78 - ETA: 12s - loss: 0.5629 - acc: 0.75 - ETA: 11s - loss: 0.5335 - acc: 0.77 - ETA: 11s - loss: 0.5384 - acc: 0.77 - ETA: 10s - loss: 0.5419 - acc: 0.76 - ETA: 9s - loss: 0.5675 - acc: 0.7461 - ETA: 8s - loss: 0.5711 - acc: 0.743 - ETA: 8s - loss: 0.5740 - acc: 0.740 - ETA: 7s - loss: 0.5563 - acc: 0.755 - ETA: 6s - loss: 0.5599 - acc: 0.752 - ETA: 5s - loss: 0.5544 - acc: 0.757 - ETA: 5s - loss: 0.5551 - acc: 0.756 - ETA: 4s - loss: 0.5507 - acc: 0.760 - ETA: 3s - loss: 0.5537 - acc: 0.757 - ETA: 2s - loss: 0.5499 - acc: 0.761 - ETA: 2s - loss: 0.5445 - acc: 0.765 - ETA: 1s - loss: 0.5474 - acc: 0.763 - ETA: 0s - loss: 0.5390 - acc: 0.770 - 16s 25ms/step - loss: 0.5410 - acc: 0.7687 - val_loss: 0.5004 - val_acc: 0.8030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ef54645978>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we train the Network.\n",
    "model.fit(X_traincv,y_train, batch_size=batch_size,epochs =50, verbose = 1,validation_data=(X_testcv,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_testcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(330),predict,c='r')\n",
    "plt.scatter(range(330),y_test,c='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
