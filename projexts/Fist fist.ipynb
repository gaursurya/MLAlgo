{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = 10\n",
    "########\n",
    "tx = range(10)\n",
    "ty = [0] * 5 + [1] * 5\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn import datasets\n",
    "\n",
    "#kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "shufflesplit = StratifiedShuffleSplit(n_splits=splits, random_state=42, test_size=4)\n",
    "\n",
    "#print(\"KFold\")\n",
    "#for train_index, test_index in kfold.split(tx, ty):\n",
    "   # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "print(\"Shuffle Split\")\n",
    "for train_index, test_index in shufflesplit.split(tx, ty):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test=pd.read_csv('test.csv')\n",
    "#df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('train.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Any Nan Value in Train dataset',df_train.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('For 1 is',160/250)\n",
    "print('For 0 is',90/250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot when target is equals to 1\n",
    "plt.plot(df_train[df_train.target==1].mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot when target is equals to 0\n",
    "plt.plot(df_train[df_train.target==0].mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myarray = np.asarray(df_train[df_train.target==1].mean(axis=1))\n",
    "#sns.distplot(myarray)\n",
    "################################################################################################\n",
    "#myarray = np.asarray(df_train[df_train.target==0].mean(axis=1))\n",
    "#sns.distplot(myarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=df_train.drop(['id','target'],axis=1).corr()\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features=df_train.drop(['id','target'],axis=1)\n",
    "cols = df_features.columns\n",
    "# Calculates pearson co-efficient for all combinations\n",
    "data_corr = df_features.corr()\n",
    "# Set the threshold to select only highly correlated attributes\n",
    "threshold = 0.25\n",
    "\n",
    "# List of pairs along with correlation above threshold\n",
    "corr_list = []\n",
    "\n",
    "size=len(df_features.columns)\n",
    "for i in range(0,size):\n",
    "    for j in range(i+1,size):\n",
    "        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\n",
    "            corr_list.append([data_corr.iloc[i,j],i,j])\n",
    "            \n",
    "#Sort to show higher ones first            \n",
    "s_corr_list = sorted(corr_list,key=lambda x: -abs(x[0]))\n",
    "\n",
    "#Print correlations and column names\n",
    "for v,i,j in s_corr_list:\n",
    "    print (\"%s and %s = %.2f\" % (cols[i],cols[j],v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(45,30))\n",
    "for i, col in enumerate(list(df_train.columns)):\n",
    "    plt.subplot(17, 50, i + 2)\n",
    "    plt.hist(df_train[col])\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(corr.var(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split--Stratified Shuffle split\n",
    "#### Taking Test size large as 80% and train size 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature=df_train.drop(['id','target'],axis=1)\n",
    "X=np.array(df_features)\n",
    "y=np.array(df_train['target'])\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=[{\"C\":np.logspace(-3,1,7)}]\n",
    "print('grid',grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "C=np.array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01,\n",
    "       4.64158883e-01, 2.15443469e+00, 1.00000000e+01])\n",
    "print('C',C)\n",
    "## n_splits ## Number of Bootstrap Splits\n",
    "## n ## Number of KFold Splits\n",
    "def Accuracy(X,y,n,n_splits):\n",
    "    l_accu_score=list()\n",
    "    key_c=list() ### \n",
    "    sss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.2, random_state=123)\n",
    "    \n",
    "    for subset_index, test_index in sss.split(X, y):\n",
    "        X_train_ss=X[subset_index]\n",
    "        Y_train_ss=y[subset_index]\n",
    "       # X_test_ss=X[test_index]\n",
    "       # Y_test_ss=y[test_index]\n",
    "        \n",
    "        for c in C:\n",
    "            #print('C',c)\n",
    "            key_c.append(c)\n",
    "            kf = KFold(n_splits =n)\n",
    "            for train_index, test_index in kf.split(X_train_ss):\n",
    "                X_train, X_test = X_train_ss[train_index], X_train_ss[test_index]\n",
    "                y_train, y_test = Y_train_ss[train_index], Y_train_ss[test_index]\n",
    "                \n",
    "                logmodel = LogisticRegression(C=c,penalty='l1',max_iter = 100000).fit(X_train,y_train)\n",
    "                ypred = logmodel.predict(X_test)\n",
    "                \n",
    "                l_accu_score.append(accuracy_score(y_test,ypred))\n",
    "                new_list = [l_accu_score[i:i+n] for i in range(0, len(l_accu_score), n)]\n",
    "                \n",
    "                #print ('Accuracy Score :',(accuracy_score(y_test,ypred)))\n",
    "    return new_list,key_c\n",
    "new_list,key_c=Accuracy(X,y,10,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_c(key_c,new_list):\n",
    "    df=pd.concat([pd.DataFrame(key_c,columns = list(\"C\")),pd.DataFrame(new_list).mean(axis=1)],axis=1)\n",
    "    return df.groupby('C').mean()\n",
    "best_c(key_c,new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "\n",
    "new_coef=list()\n",
    "accuracy=list()\n",
    "for i in range(1,1000):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=random.randint(1,1000))\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train=X[train_index]\n",
    "        Y_train=y[train_index]\n",
    "        x_test=X[test_index]\n",
    "        y_test=y[test_index]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        \n",
    "        \n",
    "        logmodel=LogisticRegression(C= .100000,penalty='l1',max_iter = 100000).fit(X_train,Y_train)\n",
    "        ypred = logmodel.predict(x_test)\n",
    "        print ('Accuracy Score :',(accuracy_score(y_test,ypred)))\n",
    "        accuracy.append(accuracy_score(y_test,ypred))\n",
    "        \n",
    "        coef=pd.Series(reduce(lambda x,y :x+y ,logmodel.coef_),df_features.columns)\n",
    "        coefficents=list(coef[coef!=0].index)\n",
    "        new_coef.append(coefficents)\n",
    "        print(coefficents)\n",
    "        \n",
    "        #new_coef = [coefficents[i:i+len(coefficents)] for i in range(0, len(coefficents),len(coefficents))]\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_coeff={}\n",
    "for wds in (range(len(new_coef))):\n",
    "    for w in new_coef[wds]:\n",
    "        if w in dict_coeff:\n",
    "            dict_coeff[w]=dict_coeff[w]+1\n",
    "        else:\n",
    "            dict_coeff[w]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum',max(accuracy))\n",
    "print('Minnimum',min(accuracy))\n",
    "print('Minnimum',reduce(lambda x, y: x + y, accuracy) / len(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_coeff=([k for (k,v) in dict_coeff.items() if v >500])\n",
    "print('Length of list',len(regular_coeff))\n",
    "#regular_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Appending id column\n",
    "regular_coeff.append('id')\n",
    "regular_coeff.append('target')\n",
    "df_train2=pd.read_csv('train.csv',usecols=regular_coeff)\n",
    "df_train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature=df_train2.drop(['id','target'],axis=1)\n",
    "X=np.array(df_feature)\n",
    "y=np.array(df_train2['target'])\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=123)\n",
    "\n",
    "accuracy_relaxed=list()\n",
    "for i in range(1,1000):\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=random.randint(1,1000))\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train=X[train_index]\n",
    "        Y_train=y[train_index]\n",
    "        x_test=X[test_index]\n",
    "        y_test=y[test_index]\n",
    "        \n",
    "        #print('length of x_test set',(np.array(x_test)).shape)\n",
    "        #print('length of X_train set',(np.array(X_train)).shape)\n",
    "        #print('length of Y_train set',(np.array(Y_train)).shape)\n",
    "        #print('length of y_test set',(np.array(y_test)).shape)\n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        logmodel_relaxed=LogisticRegression(C= .1,penalty='l1',max_iter = 100000).fit(X_train,Y_train)\n",
    "        ypred = logmodel_relaxed.predict(x_test)\n",
    "        print ('Accuracy Score :',accuracy_score(y_test,ypred))\n",
    "        accuracy_relaxed.append(accuracy_score(y_test,ypred))\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum',max(accuracy_relaxed))\n",
    "print('Minnimum',min(accuracy_relaxed))\n",
    "print('Minnimum',reduce(lambda x, y: x + y, accuracy_relaxed) / len(accuracy_relaxed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Maximum 0.92\n",
    "Minnimum 0.58\n",
    "Minnimum 0.771771771771770"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Appending id column\n",
    "regular_coeff.append('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_coeff.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('test.csv',usecols=regular_coeff)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_features=df_test.drop(['id'],axis=1)\n",
    "df_test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel_relaxed.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.array(df_test_features)\n",
    "scaler = StandardScaler()\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_test = logmodel_relaxed.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=pd.concat([df_test['id'],pd.DataFrame(ypred_test)],axis=1)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
